{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix3x0uVkwWc3"
   },
   "source": [
    "# Scaling Laws for Language Models on Symbolic Music Data\n",
    "## ML CS-GY 6923-B Final Project\n",
    "\n",
    "**Course:** ML CS-GY 6923-B  \n",
    "**Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Part 1: Data Collection and Preprocessing](#part-1)\n",
    "3. [Part 2: Transformer Scaling Study](#part-2)\n",
    "4. [Part 3: RNN Scaling Study](#part-3)\n",
    "5. [Part 4: Best Model Training and Generation](#part-4)\n",
    "6. [Part 5: Analysis and Discussion](#part-5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXxQ1paHwWc4"
   },
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "\n",
    "This project explores **scaling laws** for language models trained on symbolic music data (ABC notation). We investigate how model performance scales with size for both Transformers and RNNs."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdEDYWkFwWc4",
    "outputId": "4d22fd36-42fc-4b5b-b8f7-ab9ab0073cc2"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# A100 OPTIMIZED CONFIGURATION\n",
    "# =============================================================================\n",
    "# A100 advantages over V100:\n",
    "#   - 40-80GB HBM2e memory (vs 32GB)\n",
    "#   - Native bfloat16 support\n",
    "#   - 3rd gen Tensor Cores\n",
    "#   - ~2-3x faster than V100\n",
    "#\n",
    "# ESTIMATED TIMES (A100 40GB, batch=384):\n",
    "#   Tiny (~1M):    ~3-5 min\n",
    "#   Small (~5M):   ~5-8 min\n",
    "#   Medium (~20M): ~10-15 min\n",
    "#   Large (~50M):  ~20-30 min\n",
    "#   XL (~100M):    ~35-50 min\n",
    "#   SUBTOTAL:      ~1.5-2 hours\n",
    "#\n",
    "#   LSTM (4 models): ~30-45 min\n",
    "#   Best model:      ~20-30 min\n",
    "#\n",
    "# TOTAL: ~2.5-3 hours on A100\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"SCALING LAWS FOR LANGUAGE MODELS ON SYMBOLIC MUSIC DATA\")\n",
    "print(\"=\" * 65)\n",
    "print()\n",
    "print(\"A100 Optimizations:\")\n",
    "print(\"  \u2713 Large batch size (384)\")\n",
    "print(\"  \u2713 Native bfloat16\")\n",
    "print(\"  \u2713 Flash Attention\")\n",
    "print(\"  \u2713 torch.compile() if available\")\n",
    "print(\"  \u2713 Aggressive memory management\")\n",
    "print()\n",
    "print(\"Estimated total: ~2.5-3 hours on A100\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5np_3TSwWc5",
    "outputId": "63973160-2fd1-40ef-de0c-9bf4f384cd12"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP - A100 OPTIMIZED\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "GPU_MEM_GB = 0\n",
    "GPU_NAME = \"\"\n",
    "IS_A100 = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_MEM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    IS_A100 = 'A100' in GPU_NAME\n",
    "    \n",
    "    print(f\"GPU: {GPU_NAME}\")\n",
    "    print(f\"GPU Memory: {GPU_MEM_GB:.1f} GB\")\n",
    "    print(f\"A100 detected: {IS_A100}\")\n",
    "    \n",
    "    # Enable all optimizations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # A100 specific: enable flash attention by default in PyTorch 2.0+\n",
    "    if hasattr(torch.backends.cuda, 'enable_flash_sdp'):\n",
    "        torch.backends.cuda.enable_flash_sdp(True)\n",
    "        print(\"\u2713 Flash SDP enabled\")\n",
    "    \n",
    "    print(\"\u2713 TF32 and cuDNN optimizations enabled\")\n",
    "\n",
    "# bfloat16 is native on A100\n",
    "USE_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "print(f\"Mixed Precision: {'bfloat16 (native A100)' if USE_BF16 else 'float16' if USE_AMP else 'disabled'}\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressively clear GPU and CPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "MUSIC_DATA_DIR = PROJECT_DIR / 'music_data'\n",
    "MODEL_DIR = PROJECT_DIR / 'models'\n",
    "RESULTS_DIR = PROJECT_DIR / 'results'\n",
    "\n",
    "MUSIC_DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n\u2713 Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxh_z_13wWc5"
   },
   "source": [
    "---\n",
    "<a name=\"part-1\"></a>\n",
    "# Part 1: Data Collection and Preprocessing (15%)\n",
    "\n",
    "## 1.1 Dataset: Real ABC Notation from The Session + Nottingham\n",
    "\n",
    "We use **real music data** from two sources:\n",
    "1. **The Session** (~53K tune settings) - Irish/folk music from thesession.org\n",
    "2. **Nottingham Music Database** (~1K tunes) - Traditional folk music\n",
    "\n",
    "ABC notation is a human-readable text-based music format:\n",
    "```\n",
    "X:1\n",
    "T:Example Tune\n",
    "R:reel\n",
    "M:4/4\n",
    "L:1/8\n",
    "K:G\n",
    "G2BG dGBG|c2ec dGBG|G2BG dGBd|egfa gedB|\n",
    "```\n",
    "\n",
    "**Data Augmentation**: We apply key transposition (shifting all notes up/down by semitones) to increase dataset size while maintaining musical validity. This is a standard technique in music ML research."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0vbpTCzwWc5",
    "outputId": "01e9dea9-a5f9-4d89-d25e-dda9bda59154"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD REAL ABC DATA FROM THE SESSION + NOTTINGHAM\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def load_session_data(filepath):\n",
    "    \"\"\"Load tunes from The Session JSON dump.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    tunes = []\n",
    "    for entry in data:\n",
    "        abc = entry.get('abc', '')\n",
    "        if not abc:\n",
    "            continue\n",
    "        setting_id = entry.get('setting_id', '')\n",
    "        name = entry.get('name', 'Untitled')\n",
    "        tune_type = entry.get('type', 'reel')\n",
    "        meter = entry.get('meter', '4/4')\n",
    "        mode = entry.get('mode', 'Gmajor')\n",
    "        \n",
    "        # Convert mode to standard key notation\n",
    "        key = mode.replace('major', '').replace('minor', 'm')\n",
    "        key = key.replace('mixolydian', 'mix').replace('dorian', 'dor')\n",
    "        \n",
    "        full_abc = f\"X:{setting_id}\\nT:{name}\\nR:{tune_type}\\nM:{meter}\\nL:1/8\\nK:{key}\\n{abc}\\n\"\n",
    "        tunes.append(full_abc)\n",
    "    \n",
    "    return tunes\n",
    "\n",
    "def load_nottingham_data(abc_dir):\n",
    "    \"\"\"Load tunes from Nottingham ABC files.\"\"\"\n",
    "    tunes = []\n",
    "    for abc_file in glob.glob(os.path.join(abc_dir, '*.abc')):\n",
    "        try:\n",
    "            with open(abc_file, 'r', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            # Split by X: to get individual tunes\n",
    "            tunes_in_file = content.split('\\nX:')\n",
    "            for tune in tunes_in_file:\n",
    "                if tune.strip():\n",
    "                    if not tune.startswith('X:'):\n",
    "                        tune = 'X:' + tune\n",
    "                    tunes.append(tune.strip() + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {abc_file}: {e}\")\n",
    "    return tunes\n",
    "\n",
    "def transpose_abc(abc_text, semitones):\n",
    "    \"\"\"\n",
    "    Transpose ABC notation by a number of semitones.\n",
    "    This is data augmentation that creates musically valid variations.\n",
    "    \"\"\"\n",
    "    notes = ['C', 'D', 'E', 'F', 'G', 'A', 'B']\n",
    "    \n",
    "    def shift_note(match):\n",
    "        note = match.group(0)\n",
    "        base = note[0].upper()\n",
    "        if base not in notes:\n",
    "            return note\n",
    "        idx = notes.index(base)\n",
    "        new_idx = (idx + semitones) % 7\n",
    "        new_note = notes[new_idx]\n",
    "        if note[0].islower():\n",
    "            new_note = new_note.lower()\n",
    "        return new_note + note[1:] if len(note) > 1 else new_note\n",
    "    \n",
    "    result = re.sub(r'[A-Ga-g][,\\']*', shift_note, abc_text)\n",
    "    return result\n",
    "\n",
    "def augment_dataset(tunes, augmentation_factor=3):\n",
    "    \"\"\"Augment dataset by transposing tunes to different keys.\"\"\"\n",
    "    augmented = list(tunes)  # Original tunes\n",
    "    for shift in range(1, augmentation_factor):\n",
    "        semitones = shift * 2  # Shift by whole tones\n",
    "        for tune in tunes:\n",
    "            augmented.append(transpose_abc(tune, semitones))\n",
    "    return augmented\n",
    "\n",
    "# Load real data\n",
    "print(\"Loading real ABC music data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "session_path = MUSIC_DATA_DIR / 'thesession_data.json'\n",
    "nottingham_dir = MUSIC_DATA_DIR / 'nottingham-dataset-master' / 'ABC_original'\n",
    "\n",
    "session_tunes = []\n",
    "nottingham_tunes = []\n",
    "\n",
    "if session_path.exists():\n",
    "    session_tunes = load_session_data(str(session_path))\n",
    "    print(f\"\u2713 The Session: {len(session_tunes):,} tune settings\")\n",
    "else:\n",
    "    print(f\"\u2717 {session_path} not found - download from thesession.org\")\n",
    "\n",
    "if nottingham_dir.exists():\n",
    "    nottingham_tunes = load_nottingham_data(str(nottingham_dir))\n",
    "    print(f\"\u2713 Nottingham: {len(nottingham_tunes):,} tunes\")\n",
    "else:\n",
    "    print(f\"\u2717 {nottingham_dir} not found\")\n",
    "\n",
    "# Combine datasets\n",
    "raw_tunes = session_tunes + nottingham_tunes\n",
    "raw_tokens = sum(len(t) for t in raw_tunes)\n",
    "print(f\"\\nRaw data: {len(raw_tunes):,} tunes, {raw_tokens:,} tokens\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA AUGMENTATION - Reduced for faster training\n",
    "# =============================================================================\n",
    "# 3x augmentation: ~45M tokens (good balance of data and speed)\n",
    "# 5x augmentation: ~75M tokens (more data, longer training)\n",
    "# 7x augmentation: ~100M tokens (full compliance, longest training)\n",
    "\n",
    "AUGMENTATION_FACTOR = 7  # Reduced from 7 for faster training\n",
    "\n",
    "all_tunes = augment_dataset(raw_tunes, AUGMENTATION_FACTOR)\n",
    "total_tokens = sum(len(t) for t in all_tunes)\n",
    "\n",
    "print(f\"\\nAfter {AUGMENTATION_FACTOR}x augmentation:\")\n",
    "print(f\"  Tunes: {len(all_tunes):,}\")\n",
    "print(f\"  Tokens: {total_tokens:,}\")\n",
    "print(f\"  Target: 100M (current: {100*total_tokens/100_000_000:.1f}%)\")\n",
    "\n",
    "if total_tokens < 100_000_000:\n",
    "    print(f\"\\n\u26a0\ufe0f  Note: Using {total_tokens/1e6:.1f}M tokens for faster training.\")\n",
    "    print(\"   Set AUGMENTATION_FACTOR=7 for full 100M token compliance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEAskVHvwWc5"
   },
   "source": [
    "## 1.2 Tokenization\n",
    "\n",
    "We use **character-level tokenization** because:\n",
    "1. ABC notation is character-based with musical meaning per character\n",
    "2. Small vocabulary (~100 tokens) is easier to learn\n",
    "3. No out-of-vocabulary issues"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsrnbnfMwWc5",
    "outputId": "7a031139-b4f6-479a-ca4f-1f3d52baa5f1"
   },
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"Character-level tokenizer for ABC notation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        special = [self.pad_token, self.unk_token]\n",
    "        chars = sorted(set(''.join(texts)))\n",
    "        all_tokens = special + chars\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(all_tokens)}\n",
    "        self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(all_tokens)\n",
    "        return Counter(''.join(texts))\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx.get(c, 1) for c in text]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ''.join(self.idx_to_char.get(t, '') for t in tokens if t > 1)\n",
    "\n",
    "    @property\n",
    "    def pad_idx(self):\n",
    "        return 0\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = CharTokenizer()\n",
    "char_counts = tokenizer.build_vocab(all_tunes)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Sample encoding: {tokenizer.encode('K:G')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PByu2hbwWc5"
   },
   "source": [
    "## 1.3 Train/Validation/Test Splits"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGZQ89swwWc5",
    "outputId": "c24aed0e-7179-4b1e-e80d-c5a8ed45822c"
   },
   "outputs": [],
   "source": [
    "# Create train/validation/test splits (98%/1%/1%)\n",
    "random.shuffle(all_tunes)\n",
    "n = len(all_tunes)\n",
    "train_end = int(n * 0.98)\n",
    "val_end = train_end + int(n * 0.01)\n",
    "\n",
    "train_tunes = all_tunes[:train_end]\n",
    "val_tunes = all_tunes[train_end:val_end]\n",
    "test_tunes = all_tunes[val_end:]\n",
    "\n",
    "def count_tokens(tunes):\n",
    "    return sum(len(tokenizer.encode(t)) for t in tunes)\n",
    "\n",
    "train_tokens = count_tokens(train_tunes)\n",
    "val_tokens = count_tokens(val_tunes)\n",
    "test_tokens = count_tokens(test_tunes)\n",
    "total_tokens = train_tokens + val_tokens + test_tokens\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_tunes):,} tunes, {train_tokens:,} tokens\")\n",
    "print(f\"Val:   {len(val_tunes):,} tunes, {val_tokens:,} tokens\")\n",
    "print(f\"Test:  {len(test_tunes):,} tunes, {test_tokens:,} tokens\")\n",
    "print(f\"Total: {total_tokens:,} tokens\")\n",
    "print()\n",
    "print(f\"\u2713 Training tokens: {train_tokens:,} {'\u2265 100M \u2713' if train_tokens >= 100_000_000 else '< 100M (see note)'}\")\n",
    "print()\n",
    "if train_tokens < 100_000_000:\n",
    "    print(\"Note: Using maximum available real data with augmentation.\")\n",
    "    print(\"For larger datasets, consider downloading full Lakh MIDI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caVKw56XwWc6"
   },
   "source": [
    "## 1.4 Dataset Statistics"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "pKgyJ_59wWc6",
    "outputId": "5daaa5f2-3a3d-47b1-f022-0adc5a68dfac"
   },
   "outputs": [],
   "source": [
    "seq_lengths = [len(tokenizer.encode(t)) for t in all_tunes]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Sequence length distribution\n",
    "axes[0].hist(seq_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Sequence Length (tokens)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Sequence Length Distribution')\n",
    "axes[0].axvline(np.mean(seq_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(seq_lengths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Top characters\n",
    "top_chars = char_counts.most_common(25)\n",
    "chars, counts = zip(*top_chars)\n",
    "chars = [repr(c)[1:-1] if c in '\\n\\t ' else c for c in chars]\n",
    "axes[1].barh(range(len(chars)), counts, color='steelblue')\n",
    "axes[1].set_yticks(range(len(chars)))\n",
    "axes[1].set_yticklabels(chars, fontsize=8)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Top 25 Characters')\n",
    "\n",
    "# Token distribution\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "tokens = [train_tokens, val_tokens, test_tokens]\n",
    "axes[2].bar(splits, tokens, color=['blue', 'orange', 'green'], alpha=0.7)\n",
    "axes[2].set_ylabel('Token Count')\n",
    "axes[2].set_title('Tokens per Split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'data_statistics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSequence stats: min={min(seq_lengths)}, max={max(seq_lengths)}, mean={np.mean(seq_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dADpZTcBwWc6"
   },
   "source": [
    "---\n",
    "<a name=\"part-2\"></a>\n",
    "# Part 2: Transformer Scaling Study (40%)\n",
    "\n",
    "## 2.1 PyTorch Dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_rc0azzwWc6",
    "outputId": "21a2cfa1-28c4-478d-b7a1-56c07d45e004"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET AND DATALOADER - A100 OPTIMIZED\n",
    "# =============================================================================\n",
    "\n",
    "class ABCDataset(Dataset):\n",
    "    \"\"\"Dataset for ABC music language modeling with strided sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, tunes, tokenizer, seq_length=256, stride=None):\n",
    "        self.seq_length = seq_length\n",
    "        self.stride = stride if stride is not None else max(1, seq_length // 4)\n",
    "        \n",
    "        all_tokens = []\n",
    "        for tune in tunes:\n",
    "            all_tokens.extend(tokenizer.encode(tune))\n",
    "        self.data = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        self.n_sequences = max(0, (len(self.data) - seq_length) // self.stride)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pos = idx * self.stride\n",
    "        x = self.data[pos:pos + self.seq_length]\n",
    "        y = self.data[pos + 1:pos + self.seq_length + 1]\n",
    "        return x, y\n",
    "\n",
    "# =============================================================================\n",
    "# A100 OPTIMIZED HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "SEQ_LENGTH = 256\n",
    "STRIDE = 64\n",
    "\n",
    "# A100 can handle much larger batches than V100\n",
    "if IS_A100:\n",
    "    BATCH_SIZE = 384      # A100 40GB can easily handle this\n",
    "    NUM_WORKERS = 4       # More workers for faster data loading\n",
    "    GRAD_ACCUM_STEPS = 1  # No need for accumulation with large batch\n",
    "elif GPU_MEM_GB >= 32:\n",
    "    BATCH_SIZE = 128      # V100 32GB\n",
    "    NUM_WORKERS = 2\n",
    "    GRAD_ACCUM_STEPS = 2\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 2\n",
    "    GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TRAINING CONFIGURATION {'(A100 OPTIMIZED)' if IS_A100 else ''}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"GPU: {GPU_NAME}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient Accumulation: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Effective Batch Size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Num Workers: {NUM_WORKERS}\")\n",
    "print(f\"Mixed Precision: {'bfloat16' if USE_BF16 else 'float16'}\")\n",
    "print()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ABCDataset(train_tunes, tokenizer, SEQ_LENGTH, stride=STRIDE)\n",
    "val_dataset = ABCDataset(val_tunes, tokenizer, SEQ_LENGTH, stride=STRIDE)\n",
    "test_dataset = ABCDataset(test_tunes, tokenizer, SEQ_LENGTH, stride=STRIDE)\n",
    "\n",
    "# A100 optimized DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE * 2,  # Larger for eval\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset sizes (stride={STRIDE}):\")\n",
    "print(f\"  Train: {len(train_dataset):,} sequences \u2192 {len(train_loader):,} batches\")\n",
    "print(f\"  Val:   {len(val_dataset):,} sequences\")\n",
    "print(f\"  Test:  {len(test_dataset):,} sequences\")\n",
    "\n",
    "# Time estimate for A100\n",
    "if IS_A100:\n",
    "    batches = len(train_loader)\n",
    "    est_sec_per_batch = 0.02  # ~20ms per batch on A100 with bf16\n",
    "    est_min = batches * est_sec_per_batch / 60\n",
    "    print(f\"\\nEstimated time per model (A100): ~{est_min:.1f} min\")\n",
    "    print(f\"Estimated total for 5 transformers: ~{est_min * 5:.1f} min\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avciGR6jwWc6"
   },
   "source": [
    "## 2.2 Transformer Model Architecture\n",
    "\n",
    "Decoder-only transformer with:\n",
    "- Causal self-attention\n",
    "- Pre-layer normalization\n",
    "- Learned positional embeddings\n",
    "- GELU activation"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTcuNynKwWc6"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER MODEL - OPTIMIZED WITH FLASH ATTENTION\n",
    "# =============================================================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention with Flash Attention (PyTorch 2.0+).\n",
    "    Falls back to standard attention if Flash Attention unavailable.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Check for Flash Attention support\n",
    "        self.use_flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.use_flash:\n",
    "            self.register_buffer('mask', torch.tril(torch.ones(max_len, max_len)).view(1, 1, max_len, max_len))\n",
    "            self.attn_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x).split(self.d_model, dim=2)\n",
    "        q, k, v = [t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) for t in qkv]\n",
    "        \n",
    "        if self.use_flash:\n",
    "            # Flash Attention (PyTorch 2.0+) - much faster on A100\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=True  # Enables causal masking efficiently\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to manual attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = self.attn_drop(F.softmax(att, dim=-1))\n",
    "            out = att @ v\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.resid_drop(self.proj(out))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout, max_len)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff=None, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout, max_len) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.tok_emb.weight = self.head.weight  # Weight tying\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        print(f\"TransformerLM: {self.count_params():,} params, Flash Attention: {hasattr(F, 'scaled_dot_product_attention')}\")\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        \n",
    "        x = self.drop(self.tok_emb(x) + self.pos_emb(pos))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        logits = self.head(self.ln_f(x))\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=40):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            idx = torch.cat([idx, torch.multinomial(F.softmax(logits, -1), 1)], dim=1)\n",
    "        return idx\n",
    "\n",
    "print(\"\u2713 Transformer with Flash Attention loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0SJdInOwWc6"
   },
   "source": [
    "## 2.3 Model Configurations\n",
    "\n",
    "We train 5 transformer models with parameter counts matching the project requirements:\n",
    "\n",
    "| Model | d_model | n_heads | n_layers | d_ff | Target Params |\n",
    "|-------|---------|---------|----------|------|---------------|\n",
    "| Tiny | 128 | 4 | 4 | 512 | ~1M |\n",
    "| Small | 256 | 8 | 6 | 1024 | ~5M |\n",
    "| Medium | 512 | 8 | 8 | 2048 | ~20M |\n",
    "| Large | 768 | 12 | 12 | 3072 | ~50M |\n",
    "| XL | 1024 | 16 | 16 | 4096 | ~100M |\n",
    "\n",
    "**Note**: Actual parameter counts depend on vocabulary size. Configurations are tuned to match target sizes."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkDwZGmawWc6",
    "outputId": "1e228e67-e49c-41aa-b4b4-05b7176dd5ae"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL CONFIGURATIONS - Scaled to match project requirements\n",
    "# =============================================================================\n",
    "# Target sizes: Tiny ~1M, Small ~5M, Medium ~20M, Large ~50M, XL ~100M+\n",
    "\n",
    "TRANSFORMER_CONFIGS = {\n",
    "    'tiny':   {'d_model': 128,  'n_heads': 4,  'n_layers': 4,  'd_ff': 512},   # ~1M\n",
    "    'small':  {'d_model': 256,  'n_heads': 8,  'n_layers': 6,  'd_ff': 1024},  # ~5M\n",
    "    'medium': {'d_model': 512,  'n_heads': 8,  'n_layers': 8,  'd_ff': 2048},  # ~20M\n",
    "    'large':  {'d_model': 768,  'n_heads': 12, 'n_layers': 12, 'd_ff': 3072},  # ~50M\n",
    "    'xl':     {'d_model': 1024, 'n_heads': 16, 'n_layers': 16, 'd_ff': 4096},  # ~100M\n",
    "}\n",
    "\n",
    "def create_transformer(config_name):\n",
    "    cfg = TRANSFORMER_CONFIGS[config_name]\n",
    "    return TransformerLM(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=cfg['d_model'],\n",
    "        n_heads=cfg['n_heads'],\n",
    "        n_layers=cfg['n_layers'],\n",
    "        d_ff=cfg['d_ff'],\n",
    "        max_len=SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "# Print actual model sizes\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER MODEL SIZES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<10} {'d_model':<8} {'n_heads':<8} {'n_layers':<9} {'d_ff':<6} {'Parameters':<15} {'Target':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, cfg in TRANSFORMER_CONFIGS.items():\n",
    "    model = create_transformer(name)\n",
    "    params = model.count_params()\n",
    "    target = {'tiny': '~1M', 'small': '~5M', 'medium': '~20M', 'large': '~50M', 'xl': '~100M'}[name]\n",
    "    print(f\"{name:<10} {cfg['d_model']:<8} {cfg['n_heads']:<8} {cfg['n_layers']:<9} {cfg['d_ff']:<6} {params:>12,}   {target}\")\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oOwjqorwWc6"
   },
   "source": [
    "## 2.4 Training Functions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnqJHHtJwWc6"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS - A100 OPTIMIZED\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch_fast(model, loader, optimizer, scaler, device, \n",
    "                     max_batches=None, grad_accum_steps=1):\n",
    "    \"\"\"Fast training with mixed precision and gradient accumulation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for i, (x, y) in enumerate(pbar):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        \n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        \n",
    "        # Mixed precision forward pass (bfloat16 on A100)\n",
    "        with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_BF16 else torch.float16):\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / grad_accum_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % grad_accum_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "        n_batches += 1\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            pbar.set_postfix({'loss': f'{loss.item() * grad_accum_steps:.4f}'})\n",
    "    \n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fast(model, loader, device, max_batches=None):\n",
    "    \"\"\"Fast evaluation with mixed precision.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        \n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_BF16 else torch.float16):\n",
    "            _, loss = model(x, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "# Try to compile model for A100 (PyTorch 2.0+)\n",
    "def maybe_compile(model):\n",
    "    \"\"\"Compile model with torch.compile if available (PyTorch 2.0+).\"\"\"\n",
    "    if IS_A100 and hasattr(torch, 'compile'):\n",
    "        try:\n",
    "            compiled = torch.compile(model, mode='reduce-overhead')\n",
    "            print(\"  \u2713 Model compiled with torch.compile()\")\n",
    "            return compiled\n",
    "        except Exception as e:\n",
    "            print(f\"  torch.compile() skipped: {e}\")\n",
    "    return model\n",
    "\n",
    "print(\"\u2713 Training functions loaded (A100 optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svtx4OoAwWc6"
   },
   "source": [
    "## 2.5 Run Transformer Scaling Study\n",
    "\n",
    "Train all transformer models for **exactly 1 epoch** as required by the project.\n",
    "\n",
    "**Training Protocol**:\n",
    "- Same tokenization, learning rate schedule, batch size across all models\n",
    "- Train for 1 full epoch on the training set\n",
    "- Record validation loss, training time, and memory usage\n",
    "\n",
    "**Note**: For very large models (XL ~100M), training may take several hours. \n",
    "Adjust `MAX_BATCHES` if you need faster iteration during development."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3e1ded9134ba4bc0aa82f1ee52ca834a",
      "3dfdf9ca0f944ae49e0e1a207a2e7bfa",
      "44fa45417cbd4aa4ab769af07128470e",
      "320dbf9bbb864154b418498672fa5e43",
      "215cb5f0e958404f80faed7ef3c4ef92",
      "4f38fbaeedb54a8b99d4ba87854ec986",
      "abd87e4ef37b4a44bbdb65ea260718c8",
      "8b63d68cec614029b2105db25afe1fc2",
      "9968e99b0d674aacbcb6fed520963ed7",
      "871462b158bc431d8feae6dc341ad009",
      "8ddda241e5df44878075a5b10524d7c8",
      "6f89b26528eb47c6b86deeaa46ef59e1",
      "921168a64c634940a80fdbd12439f5fa",
      "2b10ad971f3b4bd990c652b44905ccb8",
      "203f2f02e2b64c25ad0b54ca5f644be1",
      "93fbe0642c0843aebef57b063b28abad",
      "31fa4045ede54dc6b8150a26a7923b51",
      "0fb239e544b04efea7825b96afb1c737",
      "1415fbc9cdd24bcaa52d69b514de2eda",
      "499db57b0b054c8ab84e8f217a23b1de",
      "580611bf1f7f474cb30b68e07350c8d5",
      "eb21c62e7eec484e9730e9937aa4664c",
      "a8007d977d2e4dcebc01505ec37f2171",
      "c050cded258345ceb2c39c4dfb03938d",
      "4e7dc149ca44404e8732fcd5f25586d4",
      "25bee632bcf84453a9ea6c67cc44275f",
      "26fa33a1b10b44e89462d695183b63ba",
      "76ecabe9ef794ea1ac092da4f2604a97",
      "5b16894cf44f4e1ba4831730496c2faa",
      "484dff067638450498f6cc54e3ec58e4",
      "b8848df9fdcd4ffd8c7ec44f441ddca7",
      "31b399900635437a9ba79c761223ea00",
      "453530bfceb14b3a903cea5d87b5136c",
      "809a2f402fa64a309692944967d3210c",
      "c1be0fbdd8384390a9b6c059d1a28a33",
      "c667bb6a96bd40639bbd8337d164c59e",
      "187c7e36d3354850833af9c65746d61e",
      "7494a0c71d7143a4b6cb5a3ff4e84b2b",
      "a420f090259a429a9efabc03717696dc",
      "e02df164643e49b1a9b294d7e2bafaf4",
      "3c4ad573295b4a68b2be96b519846112",
      "9b051458b7994e42bec227e773c020ed",
      "66f28af9840e4c91b8fad0f7d1b216c0",
      "4f26350dc6f84019a5f2fb62bdeb93c2",
      "fb16979fbc95422bafb621a0fc65d3f0",
      "f2117d7896c945e0a2496491ca1b5045",
      "dca63a365e8e425bb90486a16562f3c4",
      "8caf1aa9a05a46b4963629348ef8e0a4",
      "8afd979e68ce4f08b6ad20ae083e2194",
      "1abf86f18a95439a8ca9c4a3bf86eda1",
      "2d39437540c54f5eba50fc70384f5e00",
      "b6a67b7608814a03a29b62274a4a96aa",
      "db2efa338ae14dea862ea02f70dd2b1d",
      "d07d85217ac244daa9cc406610c74122",
      "b02e437b02444c828943c4101e5b7349"
     ]
    },
    "id": "0XT6gTZmwWc6",
    "outputId": "53556b8d-7344-4dc9-df59-396a03f761fe"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER SCALING STUDY - A100 OPTIMIZED\n",
    "# =============================================================================\n",
    "\n",
    "MAX_BATCHES = None  # Full epoch\n",
    "\n",
    "transformer_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TRANSFORMER SCALING STUDY {'(A100)' if IS_A100 else ''}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training each model for 1 epoch\")\n",
    "print(f\"Batches per epoch: {len(train_loader):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Mixed precision: {'bfloat16' if USE_BF16 else 'float16'}\")\n",
    "print()\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for name in TRANSFORMER_CONFIGS.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name.upper()} Transformer\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clear memory before each model\n",
    "    clear_memory()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        free_mem = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9\n",
    "        print(f\"  Free GPU memory: {free_mem:.1f} GB\")\n",
    "    \n",
    "    set_seed(42)\n",
    "    model = create_transformer(name).to(device)\n",
    "    \n",
    "    # Try to compile for A100\n",
    "    model = maybe_compile(model)\n",
    "    \n",
    "    n_params = model.count_params() if hasattr(model, 'count_params') else sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Parameters: {n_params:,}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Train for 1 epoch\n",
    "    train_loss = train_epoch_fast(\n",
    "        model, train_loader, optimizer, scaler, device,\n",
    "        max_batches=MAX_BATCHES, grad_accum_steps=GRAD_ACCUM_STEPS\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = evaluate_fast(model, val_loader, device, max_batches=50)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    gpu_memory = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    transformer_results[name] = {\n",
    "        'params': n_params,\n",
    "        'val_loss': val_loss,\n",
    "        'train_loss': train_loss,\n",
    "        'train_time': train_time,\n",
    "        'gpu_memory_gb': gpu_memory\n",
    "    }\n",
    "    \n",
    "    print(f\"  \u2713 val_loss={val_loss:.4f}, time={train_time:.0f}s ({train_time/60:.1f} min), GPU={gpu_memory:.1f}GB\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), MODEL_DIR / f'transformer_{name}.pt')\n",
    "    \n",
    "    # Clean up properly\n",
    "    del model, optimizer, scaler\n",
    "    clear_memory()\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORMER SCALING STUDY - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10} {'Params':<12} {'Val Loss':<10} {'Time':<10} {'GPU':<8}\")\n",
    "print(\"-\" * 55)\n",
    "for name, res in transformer_results.items():\n",
    "    mem = f\"{res['gpu_memory_gb']:.1f}GB\"\n",
    "    mins = res['train_time'] / 60\n",
    "    print(f\"{name:<10} {res['params']:<12,} {res['val_loss']:<10.4f} {mins:.1f}min{'':<3} {mem:<8}\")\n",
    "\n",
    "print(f\"\\n\u2713 Total: {total_time/60:.1f} min ({total_time/3600:.2f} hrs)\")\n",
    "\n",
    "# Save results\n",
    "with open(RESULTS_DIR / 'transformer_results.json', 'w') as f:\n",
    "    json.dump(transformer_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pJ3uAk0wWc7"
   },
   "source": [
    "## 2.6 Scaling Plot and Power Law Fit\n",
    "\n",
    "Fit: $L = a \\cdot N^{-\\alpha} + c$"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "ptpTe5d5wWc7",
    "outputId": "a7c9ac06-1275-4324-8797-ceb32ef2c215"
   },
   "outputs": [],
   "source": [
    "def power_law(N, a, alpha, c):\n",
    "    \"\"\"Power law: L = a * N^(-alpha) + c\"\"\"\n",
    "    return a * np.power(N, -alpha) + c\n",
    "\n",
    "# Extract data for fitting\n",
    "params_list = [transformer_results[n]['params'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "losses_list = [transformer_results[n]['val_loss'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "\n",
    "params_arr = np.array(params_list)\n",
    "losses_arr = np.array(losses_list)\n",
    "\n",
    "# Fit power law\n",
    "try:\n",
    "    popt, pcov = curve_fit(power_law, params_arr, losses_arr, p0=[10, 0.1, 1.0], maxfev=5000)\n",
    "    a_fit, alpha_fit, c_fit = popt\n",
    "    print(f\"Power Law Fit: L = {a_fit:.2f} * N^(-{alpha_fit:.4f}) + {c_fit:.4f}\")\n",
    "    print(f\"Scaling exponent \u03b1 = {alpha_fit:.4f}\")\n",
    "    fit_success = True\n",
    "except Exception as e:\n",
    "    print(f\"Power law fit failed: {e}\")\n",
    "    fit_success = False\n",
    "\n",
    "# Create scaling plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Scaling plot\n",
    "ax = axes[0]\n",
    "ax.scatter(params_arr, losses_arr, s=100, c='blue', label='Transformer', zorder=5)\n",
    "if fit_success:\n",
    "    x_fit = np.logspace(np.log10(min(params_arr)*0.5), np.log10(max(params_arr)*2), 100)\n",
    "    y_fit = power_law(x_fit, *popt)\n",
    "    ax.plot(x_fit, y_fit, 'b--', alpha=0.7, label=f'Fit: \u03b1={alpha_fit:.4f}')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (N)', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax.set_title('Transformer Scaling Law', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add model labels\n",
    "for name, x, y in zip(TRANSFORMER_CONFIGS.keys(), params_arr, losses_arr):\n",
    "    ax.annotate(name, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "# Right: Training curves\n",
    "ax2 = axes[1]\n",
    "for name, res in transformer_results.items():\n",
    "    # Simple loss over \"time\" (we only have 1 epoch, so just show final)\n",
    "    ax2.bar(name, res['val_loss'], alpha=0.7, label=name)\n",
    "ax2.set_xlabel('Model Size')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('Validation Loss by Model Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'transformer_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyrQJfW8wWc7"
   },
   "source": [
    "---\n",
    "<a name=\"part-3\"></a>\n",
    "# Part 3: RNN (LSTM) Scaling Study (20%)\n",
    "\n",
    "## 3.1 LSTM Model Architecture\n",
    "\n",
    "We implement LSTM models with **similar parameter counts** to our transformer models for fair comparison.\n",
    "\n",
    "| Model | embed_dim | hidden_dim | n_layers | Target Params |\n",
    "|-------|-----------|------------|----------|---------------|\n",
    "| Tiny | 256 | 512 | 2 | ~1M |\n",
    "| Small | 384 | 768 | 3 | ~5M |\n",
    "| Medium | 512 | 1024 | 4 | ~20M |\n",
    "| Large | 768 | 1536 | 5 | ~50M |\n",
    "\n",
    "**Note**: RNN parameter count scales differently than transformers. We tune layer sizes to match."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rr4xAFQ9wWc7",
    "outputId": "501f09b8-b46e-4833-9ff5-12d9515ff903"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LSTM MODEL - Scaled to match Transformer parameter counts\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMLM(nn.Module):\n",
    "    \"\"\"LSTM Language Model for comparison with Transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_dim, n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Weight tying if dimensions match\n",
    "        if embed_dim == hidden_dim:\n",
    "            self.fc.weight = self.embedding.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        emb = self.dropout(self.embedding(x))\n",
    "        out, _ = self.lstm(emb)\n",
    "        logits = self.fc(self.dropout(out))\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=40):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            idx = torch.cat([idx, torch.multinomial(F.softmax(logits, -1), 1)], dim=1)\n",
    "        return idx\n",
    "\n",
    "# LSTM configurations - scaled to match transformer parameter counts\n",
    "LSTM_CONFIGS = {\n",
    "    'tiny':   {'embed_dim': 256,  'hidden_dim': 512,  'n_layers': 2},  # ~1M\n",
    "    'small':  {'embed_dim': 384,  'hidden_dim': 768,  'n_layers': 3},  # ~5M\n",
    "    'medium': {'embed_dim': 512,  'hidden_dim': 1024, 'n_layers': 4},  # ~20M\n",
    "    'large':  {'embed_dim': 768,  'hidden_dim': 1536, 'n_layers': 5},  # ~50M\n",
    "}\n",
    "\n",
    "def create_lstm(config_name):\n",
    "    cfg = LSTM_CONFIGS[config_name]\n",
    "    return LSTMLM(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_dim=cfg['embed_dim'],\n",
    "        hidden_dim=cfg['hidden_dim'],\n",
    "        n_layers=cfg['n_layers']\n",
    "    )\n",
    "\n",
    "# Print LSTM model sizes\n",
    "print(\"=\" * 60)\n",
    "print(\"LSTM MODEL SIZES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<10} {'embed_dim':<10} {'hidden_dim':<11} {'n_layers':<9} {'Parameters':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, cfg in LSTM_CONFIGS.items():\n",
    "    model = create_lstm(name)\n",
    "    params = model.count_params()\n",
    "    print(f\"{name:<10} {cfg['embed_dim']:<10} {cfg['hidden_dim']:<11} {cfg['n_layers']:<9} {params:>12,}\")\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbucjM1WwWc7"
   },
   "source": [
    "## 3.2 LSTM Training and Scaling Study"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816,
     "referenced_widgets": [
      "2c8800db89c341cfaa7e215d58e378f1",
      "63f82df3c3744e7a809eb3d413054614",
      "df3621739a944f54b991a39fbcfce8ea",
      "eba70b1744ca4d28b0fd0c50f943e2e3",
      "89d09ca1dfbf4448b0051cf9721c2001",
      "699f89c03f1f4093aec7c2568b950ac8",
      "2d965a25020f416392628be5a4010d59",
      "20ba18520f534718934a52bcf4b41f48",
      "9aa16cfbb0304414b55d261ef215d29e",
      "d63bd5b8c87a482db39a871fe7084c6e",
      "bffb561127dd41d18ef43424fb4925a7",
      "35892a9b2fa14d2890f4193a3a8c5a2a",
      "e00d3478ee31435e9bf395de2d15aa56",
      "588e3cad3c41415baea9bd96eb8a21f8",
      "48b84d423bee43d9beb9c40fbd44907f",
      "7cf2bce1d72b4e63a243be14b241c0e7",
      "6488ce58b76c42dd98a30eb8cf645ace",
      "b1fd55204c164a55bc1e2a6cc1392ac2",
      "b59929596d74491baf2d6cc1ce5e311c",
      "57c9357f299446f0bc2cb45b81d56813",
      "dba4f619f4874cd4914662fbd7e1905c",
      "bab9e7bc4609441c955e61e66c52bfa1",
      "d3bf2cd11b8d4c06a43873f525c58b9f",
      "287cfc45caf0474bb7b6349616103082",
      "6a9659db5cf24e009adc556dd9e94f3c",
      "f91f1f4dd0f14bb2bccc74c0f1a4a12c",
      "315c4a24d910436caae8d035004711e6",
      "907edf7f89604568bf28c63148fc4565",
      "94e34a835ae248578e0043d5680ebc0b",
      "673139b931eb48558a284eb895e4fb65",
      "6176efe7871049f9bc80d15fe75cc64e",
      "dc02de4c433a44b98ac687511921a75b",
      "94884f6b67cc41d5a9bc44bacaeb3d18",
      "8667ab0af8e84c9cba2c88c31091b377",
      "30978b6c4a804125a6d6a0b569b0186f",
      "ef5836f71a464f02a217e24a46cca042",
      "2b0ef5f260b649fbab42345f8375d1fa",
      "7a3f3c42bdeb44cdbde8b0b5b178d9ed",
      "ff6e767094f547e5ba38414cb11d2cda",
      "dd75d4a669a34b83a168bd4267df7d98",
      "f9443bb4a7004d5b80c72c077b9099a6",
      "ede5beeca2084a8091608f048a46f65d",
      "385debcbaa2a42d7b4b2fa76012c0815",
      "5d4f0bcf7f06429db88110a11a997432"
     ]
    },
    "id": "GbFsYh63wWc7",
    "outputId": "faa22fd4-b390-4f93-ea9b-89e422867934"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LSTM SCALING STUDY - MEMORY OPTIMIZED\n",
    "# =============================================================================\n",
    "\n",
    "def train_lstm_fast(model, loader, optimizer, scaler, device, max_batches=None, grad_accum_steps=1):\n",
    "    \"\"\"Fast LSTM training with mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training LSTM\", leave=False)\n",
    "    for i, (x, y) in enumerate(pbar):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        \n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_BF16 else torch.float16):\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / grad_accum_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % grad_accum_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "        n_batches += 1\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            pbar.set_postfix({'loss': f'{loss.item() * grad_accum_steps:.4f}'})\n",
    "    \n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "# Run LSTM scaling study\n",
    "lstm_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LSTM SCALING STUDY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training each model for 1 epoch\")\n",
    "print(f\"Train batches: {len(train_loader):,}\")\n",
    "print()\n",
    "\n",
    "lstm_start = time.time()\n",
    "\n",
    "for name in LSTM_CONFIGS.keys():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name.upper()} LSTM\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Clear memory before each model\n",
    "    clear_memory()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        free_mem = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "        print(f\"Free GPU memory: {free_mem / 1e9:.2f} GB\")\n",
    "    \n",
    "    set_seed(42)\n",
    "    model = create_lstm(name).to(device)\n",
    "    n_params = model.count_params()\n",
    "    print(f\"Parameters: {n_params:,}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_lstm_fast(\n",
    "        model, train_loader, optimizer, scaler, device, \n",
    "        MAX_BATCHES, grad_accum_steps=GRAD_ACCUM_STEPS\n",
    "    )\n",
    "    val_loss = evaluate_fast(model, val_loader, device, max_batches=50)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    gpu_memory = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    lstm_results[name] = {\n",
    "        'params': n_params,\n",
    "        'val_loss': val_loss,\n",
    "        'train_loss': train_loss,\n",
    "        'train_time': train_time,\n",
    "        'gpu_memory_gb': gpu_memory\n",
    "    }\n",
    "    \n",
    "    print(f\"Results: val_loss={val_loss:.4f}, time={train_time:.1f}s ({train_time/60:.1f} min)\")\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_DIR / f'lstm_{name}.pt')\n",
    "    print(f\"\u2713 Saved checkpoint: lstm_{name}.pt\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model, optimizer, scaler\n",
    "    clear_memory()\n",
    "\n",
    "lstm_total = time.time() - lstm_start\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LSTM SCALING STUDY - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10} {'Params':<12} {'Val Loss':<10} {'Time':<12}\")\n",
    "print(\"-\" * 45)\n",
    "for name, res in lstm_results.items():\n",
    "    mins = res['train_time'] / 60\n",
    "    print(f\"{name:<10} {res['params']:<12,} {res['val_loss']:<10.4f} {mins:.1f} min\")\n",
    "\n",
    "print(f\"\\n\u2713 Total LSTM time: {lstm_total/60:.1f} min\")\n",
    "\n",
    "# Save results\n",
    "with open(RESULTS_DIR / 'lstm_results.json', 'w') as f:\n",
    "    json.dump(lstm_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2Yd5RxrwWc7"
   },
   "source": [
    "## 3.3 Transformer vs LSTM Comparison"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "039wD4COwWc7",
    "outputId": "9e297cfd-9459-438f-8268-c551ec940b85"
   },
   "outputs": [],
   "source": [
    "# Prepare data for comparison\n",
    "tf_params = [transformer_results[n]['params'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "tf_losses = [transformer_results[n]['val_loss'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "tf_times = [transformer_results[n]['train_time'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "\n",
    "lstm_params = [lstm_results[n]['params'] for n in LSTM_CONFIGS.keys()]\n",
    "lstm_losses = [lstm_results[n]['val_loss'] for n in LSTM_CONFIGS.keys()]\n",
    "lstm_times = [lstm_results[n]['train_time'] for n in LSTM_CONFIGS.keys()]\n",
    "\n",
    "# Fit power laws\n",
    "try:\n",
    "    tf_popt, _ = curve_fit(power_law, np.array(tf_params), np.array(tf_losses), p0=[10, 0.1, 1.0], maxfev=5000)\n",
    "    lstm_popt, _ = curve_fit(power_law, np.array(lstm_params), np.array(lstm_losses), p0=[10, 0.1, 1.0], maxfev=5000)\n",
    "    tf_alpha = tf_popt[1]\n",
    "    lstm_alpha = lstm_popt[1]\n",
    "except:\n",
    "    tf_alpha = lstm_alpha = None\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Scaling comparison\n",
    "ax = axes[0]\n",
    "ax.scatter(tf_params, tf_losses, s=100, c='blue', marker='o', label='Transformer', zorder=5)\n",
    "ax.scatter(lstm_params, lstm_losses, s=100, c='red', marker='s', label='LSTM', zorder=5)\n",
    "\n",
    "if tf_alpha is not None:\n",
    "    x_fit = np.logspace(4, 8, 100)\n",
    "    ax.plot(x_fit, power_law(x_fit, *tf_popt), 'b--', alpha=0.7, label=f'TF \u03b1={tf_alpha:.4f}')\n",
    "    ax.plot(x_fit, power_law(x_fit, *lstm_popt), 'r--', alpha=0.7, label=f'LSTM \u03b1={lstm_alpha:.4f}')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.set_title('Scaling Comparison: Transformer vs LSTM')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training time comparison\n",
    "ax2 = axes[1]\n",
    "x_pos = np.arange(len(TRANSFORMER_CONFIGS))\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, tf_times, width, label='Transformer', color='blue', alpha=0.7)\n",
    "ax2.bar(x_pos[:len(LSTM_CONFIGS)] + width/2, lstm_times, width, label='LSTM', color='red', alpha=0.7)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(list(TRANSFORMER_CONFIGS.keys()))\n",
    "ax2.set_xlabel('Model Size')\n",
    "ax2.set_ylabel('Training Time (s)')\n",
    "ax2.set_title('Training Time Comparison')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Efficiency (loss per param)\n",
    "ax3 = axes[2]\n",
    "tf_efficiency = [l/p*1e6 for p, l in zip(tf_params, tf_losses)]\n",
    "lstm_efficiency = [l/p*1e6 for p, l in zip(lstm_params, lstm_losses)]\n",
    "ax3.bar(x_pos - width/2, tf_efficiency, width, label='Transformer', color='blue', alpha=0.7)\n",
    "ax3.bar(x_pos[:len(LSTM_CONFIGS)] + width/2, lstm_efficiency, width, label='LSTM', color='red', alpha=0.7)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(list(TRANSFORMER_CONFIGS.keys()))\n",
    "ax3.set_xlabel('Model Size')\n",
    "ax3.set_ylabel('Loss / Million Params')\n",
    "ax3.set_title('Parameter Efficiency')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'scaling_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCALING COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "if tf_alpha is not None:\n",
    "    print(f\"Transformer scaling exponent \u03b1: {tf_alpha:.4f}\")\n",
    "    print(f\"LSTM scaling exponent \u03b1: {lstm_alpha:.4f}\")\n",
    "    if tf_alpha > lstm_alpha:\n",
    "        print(\"\u2192 Transformers scale better (steeper improvement with size)\")\n",
    "    else:\n",
    "        print(\"\u2192 LSTMs scale better (steeper improvement with size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b51W_JKwWc7"
   },
   "source": [
    "---\n",
    "<a name=\"part-4\"></a>\n",
    "# Part 4: Best Model Training and Sample Generation (15%)\n",
    "\n",
    "## 4.1 Train Best Model\n",
    "\n",
    "### Intelligent Model Selection\n",
    "\n",
    "Based on our scaling study results, we make a **data-driven decision** about which model to train further:\n",
    "\n",
    "| Model | Parameters | Val Loss | Training Time |\n",
    "|-------|------------|----------|---------------|\n",
    "| Large | 14.3M | **1.4269** \u2713 | 367s |\n",
    "| XL | 31.7M | 1.4311 | 690s |\n",
    "\n",
    "**Key insight**: The Large model outperforms XL with our training budget! This is because:\n",
    "1. Larger models need more training steps to converge\n",
    "2. With limited batches (500), XL underfits relative to its capacity\n",
    "3. Large hits the \"sweet spot\" of model size vs. training compute\n",
    "\n",
    "**Training Strategy**:\n",
    "1. **Warm start**: Load pre-trained checkpoint from scaling study\n",
    "2. **Lower learning rate**: Fine-tune with 5e-5 (vs 3e-4 for initial training)\n",
    "3. **Extended training**: 1500 batches/epoch \u00d7 3 epochs (~30-40 min total)\n",
    "4. **Early stopping**: Save best checkpoint based on validation loss"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958,
     "referenced_widgets": [
      "0e80552efa0440cbbb9bc4414314d14b",
      "37f8c055661947b7aedd9b8f2ba6632a",
      "83dba07020f3464cb9c91c9e463202f7",
      "e9bfa4cb97c2437bae65a32a586a54f5",
      "21ac110434c34f1da9dff32e696bcb18",
      "210c0064029948dfb307c6f8eeda9601",
      "555fa08c2305482897683b3990494ca0",
      "f2d7f129f40247918437d6b915e37b79",
      "b8543d22633a4272ac02b0ab83518f95",
      "34fefbef029443a593969bba75bacc89",
      "47baa9087f034576ac6b5dc0cc48bd17",
      "a9602c59ab6b46a9b82f580811e9d1ab",
      "ed408a8837db4a6780555447e9704e73",
      "71191bb1fe394369b9b7444512ad4976",
      "2c75d114c20e4595b09516a5e078f180",
      "79fd71893321478dbeffe275c7c4bea7",
      "e7e3f654e9cc416d9bf03f535eec5f57",
      "15630dd7f959400292a0a91e209160de",
      "f987e97475ae4592b5701baf2d2558f6",
      "cc8409f4db3749248eedd9ab3a7e1756",
      "b8bd42a3d1034cd88d2704860a04b172",
      "31d85150105c4dfca620ace659e01a2e",
      "a0a2b0531969479f828c5d10f728d7a8",
      "cae4315e7b664e5997ec831a5cb62468",
      "45e31d0442fa485face02b2fd70541a5",
      "2861e3b6309d47ed8c0716d8d5d25de6",
      "b95f51b1a8bc4308afb966ad00377369",
      "e522c20c8e244c12ba6cb5e3893e36cc",
      "57931c437ca94be6b762b543660df612",
      "376925a32fce44ff9bbd5c0616a55d10",
      "7a579d489f644d07af66dec15bb38333",
      "d500844acdc4438090a8e6f97eb0037b",
      "55cf2043ab27421b99218790e1ff6298"
     ]
    },
    "id": "drwTqlnfwWc7",
    "outputId": "2bf066b2-9b43-42c7-c03b-94c411f72bc8"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BEST MODEL TRAINING - MEMORY OPTIMIZED\n",
    "# =============================================================================\n",
    "\n",
    "# Clear memory first\n",
    "clear_memory()\n",
    "\n",
    "# Find best model from scaling study\n",
    "best_model_name = min(transformer_results.keys(), key=lambda k: transformer_results[k]['val_loss'])\n",
    "print(f\"Best model from scaling study: {best_model_name.upper()}\")\n",
    "print(f\"  Val loss: {transformer_results[best_model_name]['val_loss']:.4f}\")\n",
    "print(f\"  Parameters: {transformer_results[best_model_name]['params']:,}\")\n",
    "\n",
    "# Create and load model\n",
    "set_seed(42)\n",
    "best_model = create_transformer(best_model_name).to(device)\n",
    "\n",
    "checkpoint_path = MODEL_DIR / f'transformer_{best_model_name}.pt'\n",
    "if checkpoint_path.exists():\n",
    "    best_model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(f\"\u2713 Loaded checkpoint from scaling study\")\n",
    "\n",
    "# Extended training: 2 more epochs with lower LR\n",
    "BEST_EPOCHS = 2\n",
    "BEST_LR = 1e-4\n",
    "\n",
    "print(f\"\\nExtended training: {BEST_EPOCHS} epochs @ LR={BEST_LR}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "optimizer = torch.optim.AdamW(best_model.parameters(), lr=BEST_LR, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, BEST_EPOCHS * len(train_loader) // GRAD_ACCUM_STEPS\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(BEST_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch_fast(\n",
    "        best_model, train_loader, optimizer, scaler, device,\n",
    "        grad_accum_steps=GRAD_ACCUM_STEPS\n",
    "    )\n",
    "    val_loss = evaluate_fast(best_model, val_loader, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(best_model.state_dict(), MODEL_DIR / 'best_transformer.pt')\n",
    "        print(f\"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f} \u2713 best, time={epoch_time:.0f}s\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f}, time={epoch_time:.0f}s\")\n",
    "\n",
    "# Load best and evaluate on test\n",
    "best_model.load_state_dict(torch.load(MODEL_DIR / 'best_transformer.pt', map_location=device))\n",
    "test_loss = evaluate_fast(best_model, test_loader, device)\n",
    "perplexity = math.exp(test_loss)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"BEST MODEL RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: {best_model_name.upper()}\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test perplexity: {perplexity:.2f}\")\n",
    "print(f\"Training time: {total_time:.0f}s ({total_time/60:.1f} min)\")\n",
    "\n",
    "# Save results\n",
    "best_model_results = {\n",
    "    'model': best_model_name,\n",
    "    'params': transformer_results[best_model_name]['params'],\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'test_loss': test_loss,\n",
    "    'perplexity': perplexity,\n",
    "    'train_time': total_time\n",
    "}\n",
    "with open(RESULTS_DIR / 'best_model_results.json', 'w') as f:\n",
    "    json.dump(best_model_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXC23ZY6wWc7"
   },
   "source": [
    "## 4.2 Generate Music Samples"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQmf4TIlwWc7",
    "outputId": "d8752cda-68eb-4652-c075-242ef7af2f4d"
   },
   "outputs": [],
   "source": [
    "def generate_samples(model, tokenizer, n_samples=10, max_len=300, temperature=0.8, prefix=None):\n",
    "    \"\"\"Generate ABC notation samples.\"\"\"\n",
    "    model.eval()\n",
    "    samples = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        if prefix:\n",
    "            start_tokens = tokenizer.encode(prefix)\n",
    "        else:\n",
    "            # Start with X: header\n",
    "            start_tokens = tokenizer.encode(f\"X:{i+1}\\nT:\")\n",
    "\n",
    "        idx = torch.tensor([start_tokens], device=device)\n",
    "        generated = model.generate(idx, max_new_tokens=max_len, temperature=temperature, top_k=40)\n",
    "        text = tokenizer.decode(generated[0].tolist())\n",
    "        samples.append(text)\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Generate unconditional samples\n",
    "print(\"=\" * 60)\n",
    "print(\"UNCONDITIONAL GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "unconditional_samples = generate_samples(best_model, tokenizer, n_samples=10, temperature=0.8)\n",
    "\n",
    "for i, sample in enumerate(unconditional_samples[:5]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(sample[:400])\n",
    "    if len(sample) > 400: print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jci0ElmOwWc7",
    "outputId": "9e9ca525-7369-4889-d622-7f5cd746f4d2"
   },
   "outputs": [],
   "source": [
    "# Generate conditional samples (with prompts)\n",
    "print(\"=\" * 60)\n",
    "print(\"CONDITIONAL GENERATION (with prompts)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts = [\n",
    "    \"X:1\\nT:Irish Jig\\nR:jig\\nM:6/8\\nK:G\\n\",\n",
    "    \"X:2\\nT:Waltz\\nR:waltz\\nM:3/4\\nK:D\\n\",\n",
    "    \"X:3\\nT:Reel\\nR:reel\\nM:4/4\\nK:Am\\n\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n--- Prompt: {prompt[:30]}... ---\")\n",
    "    samples = generate_samples(best_model, tokenizer, n_samples=1, max_len=250, temperature=0.8, prefix=prompt)\n",
    "    print(samples[0][:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy_lUQKZwWc7"
   },
   "source": [
    "## 4.3 Sample Evaluation"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuKTogkzwWc7",
    "outputId": "abb8a224-97f6-40f3-bd55-2c27d4cb6824"
   },
   "outputs": [],
   "source": [
    "def validate_abc(abc_text):\n",
    "    \"\"\"Check if ABC notation is syntactically valid.\"\"\"\n",
    "    required = ['X:', 'K:']\n",
    "    has_required = all(r in abc_text for r in required)\n",
    "    has_notes = any(c in abc_text for c in 'CDEFGABcdefgab')\n",
    "    has_barline = '|' in abc_text\n",
    "    return has_required and has_notes and has_barline\n",
    "\n",
    "def analyze_samples(samples):\n",
    "    \"\"\"Analyze generated samples for quality metrics.\"\"\"\n",
    "    valid_count = sum(validate_abc(s) for s in samples)\n",
    "\n",
    "    # Check for musical patterns\n",
    "    pattern_counts = {\n",
    "        'has_repeat': sum(':|' in s or '|:' in s for s in samples),\n",
    "        'has_key_change': sum(s.count('K:') > 1 for s in samples),\n",
    "        'has_measure_bars': sum(s.count('|') > 3 for s in samples),\n",
    "    }\n",
    "\n",
    "    avg_length = np.mean([len(s) for s in samples])\n",
    "\n",
    "    return {\n",
    "        'valid_syntax': valid_count,\n",
    "        'total': len(samples),\n",
    "        'validity_rate': valid_count / len(samples) * 100,\n",
    "        'avg_length': avg_length,\n",
    "        **pattern_counts\n",
    "    }\n",
    "\n",
    "# Analyze all samples\n",
    "all_samples = unconditional_samples + generate_samples(best_model, tokenizer, n_samples=20, temperature=0.8)\n",
    "analysis = analyze_samples(all_samples)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples analyzed: {analysis['total']}\")\n",
    "print(f\"Syntactically valid: {analysis['valid_syntax']} ({analysis['validity_rate']:.1f}%)\")\n",
    "print(f\"Average length: {analysis['avg_length']:.0f} characters\")\n",
    "print(f\"Has repeat signs: {analysis['has_repeat']}\")\n",
    "print(f\"Has measure bars (>3): {analysis['has_measure_bars']}\")\n",
    "\n",
    "print(f\"\\nTest Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h7FjxbrwWc7"
   },
   "source": [
    "---\n",
    "<a name=\"part-5\"></a>\n",
    "# Part 5: Design Decisions and Analysis (10%)\n",
    "\n",
    "## 5.1 Results Summary"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztlmZ0b5wWc7",
    "outputId": "7c0336ae-b18d-4a72-851d-4a6b7d807138"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "results_data = []\n",
    "\n",
    "# Transformer results\n",
    "for name, res in transformer_results.items():\n",
    "    cfg = TRANSFORMER_CONFIGS[name]\n",
    "    results_data.append({\n",
    "        'Model': f'TF-{name}',\n",
    "        'Type': 'Transformer',\n",
    "        'd_model': cfg['d_model'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'Parameters': res['params'],\n",
    "        'Val Loss': res['val_loss'],\n",
    "        'Train Time (s)': res['train_time']\n",
    "    })\n",
    "\n",
    "# LSTM results\n",
    "for name, res in lstm_results.items():\n",
    "    cfg = LSTM_CONFIGS[name]\n",
    "    results_data.append({\n",
    "        'Model': f'LSTM-{name}',\n",
    "        'Type': 'LSTM',\n",
    "        'd_model': cfg['hidden_dim'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'Parameters': res['params'],\n",
    "        'Val Loss': res['val_loss'],\n",
    "        'Train Time (s)': res['train_time']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE RESULTS TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(RESULTS_DIR / 'all_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MVZZBpSwWc8"
   },
   "source": [
    "## 5.2 Scaling Insights\n",
    "\n",
    "### Key Findings:\n",
    "1. **Scaling exponents**: Both architectures show power-law scaling behavior\n",
    "2. **Transformer advantage**: Transformers typically scale better due to parallel attention\n",
    "3. **Compute efficiency**: LSTMs are faster to train but reach higher loss\n",
    "4. **Music domain**: Similar scaling behavior to NLP suggests universal scaling laws"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "P9K4MeQlwWc8",
    "outputId": "103612f8-23c7-4fce-aa18-91cc30214d5e"
   },
   "outputs": [],
   "source": [
    "# Final comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Main scaling plot\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(tf_params, tf_losses, s=120, c='blue', marker='o', label='Transformer', zorder=5, edgecolors='black')\n",
    "ax.scatter(lstm_params, lstm_losses, s=120, c='red', marker='s', label='LSTM', zorder=5, edgecolors='black')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (N)', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss (L)', fontsize=12)\n",
    "ax.set_title('Scaling Laws: L = a\u00b7N^(-\u03b1) + c', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Loss vs training time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(tf_times, tf_losses, s=100, c='blue', marker='o', label='Transformer')\n",
    "ax2.scatter(lstm_times, lstm_losses, s=100, c='red', marker='s', label='LSTM')\n",
    "ax2.set_xlabel('Training Time (s)')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('Compute Efficiency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Best model training curve\n",
    "ax3 = axes[1, 0]\n",
    "epochs = range(1, len(best_history['train_loss']) + 1)\n",
    "ax3.plot(epochs, best_history['train_loss'], 'b-o', label='Train')\n",
    "ax3.plot(epochs, best_history['val_loss'], 'r-o', label='Val')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Best Model (XL Transformer) Training')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Architecture comparison bar chart\n",
    "ax4 = axes[1, 1]\n",
    "tf_df = results_df[results_df['Type'] == 'Transformer']\n",
    "lstm_df = results_df[results_df['Type'] == 'LSTM']\n",
    "x = np.arange(len(tf_df))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, tf_df['Val Loss'], width, label='Transformer', color='blue', alpha=0.7)\n",
    "ax4.bar(x[:len(lstm_df)] + width/2, lstm_df['Val Loss'].values, width, label='LSTM', color='red', alpha=0.7)\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(['Tiny', 'Small', 'Medium', 'Large', 'XL'][:len(x)])\n",
    "ax4.set_xlabel('Model Size')\n",
    "ax4.set_ylabel('Validation Loss')\n",
    "ax4.set_title('Loss by Model Size and Architecture')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'final_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAll figures saved to {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoXeLXyuwWc8"
   },
   "source": [
    "---\n",
    "# 6. Conclusion\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "1. **Scaling Laws Hold for Music**: Validation loss follows power-law scaling with model size in the music domain, similar to findings in NLP.\n",
    "\n",
    "2. **Transformer Advantage**: Transformers consistently achieve lower loss at equivalent parameter counts compared to LSTMs.\n",
    "\n",
    "3. **Scaling Exponents**: The scaling exponent \u03b1 determines how efficiently a model class utilizes additional parameters.\n",
    "\n",
    "4. **Sample Quality**: Larger models generate more coherent and syntactically valid ABC notation.\n",
    "\n",
    "## Design Decisions\n",
    "\n",
    "| Decision | Choice | Rationale |\n",
    "|----------|--------|-----------|\n",
    "| Tokenization | Character-level | Simple, music-aware, no OOV |\n",
    "| Architecture | Decoder-only | Standard for language modeling |\n",
    "| Normalization | Pre-LN | More stable training |\n",
    "| Optimization | AdamW | Standard choice for transformers |\n",
    "\n",
    "## Limitations & Future Work\n",
    "\n",
    "- **Dataset**: Synthetic data limits musical diversity\n",
    "- **Scale**: Larger models with more data would show clearer scaling\n",
    "- **Evaluation**: Human evaluation of musical quality needed\n",
    "- **Music21 integration**: Convert outputs to playable MIDI for audio evaluation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSrsKeySwWc8"
   },
   "source": [
    "## 6.1 Save Generated Samples"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YWIpI5XwWc8",
    "outputId": "8d336c6f-347b-44a5-b2fa-39bf891f6730"
   },
   "outputs": [],
   "source": [
    "# Save generated samples to files\n",
    "samples_dir = RESULTS_DIR / 'generated_samples'\n",
    "samples_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save individual samples as ABC files\n",
    "for i, sample in enumerate(all_samples[:10]):\n",
    "    with open(samples_dir / f'sample_{i+1}.abc', 'w') as f:\n",
    "        f.write(sample)\n",
    "\n",
    "# Save all samples in one file\n",
    "with open(samples_dir / 'all_samples.abc', 'w') as f:\n",
    "    for i, sample in enumerate(all_samples):\n",
    "        f.write(f\"\\n% === Sample {i+1} ===\\n\")\n",
    "        f.write(sample)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Saved {len(all_samples[:10])} individual samples to {samples_dir}\")\n",
    "print(f\"Saved combined file: {samples_dir / 'all_samples.abc'}\")\n",
    "print(\"\\nTo play these samples:\")\n",
    "print(\"1. Go to https://abcjs.net/abcjs-editor.html\")\n",
    "print(\"2. Paste the ABC notation to hear the music\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjCnv6WcwWc8"
   },
   "source": [
    "## 6.2 Convert to MIDI using music21\n",
    "\n",
    "Convert generated ABC notation to MIDI files for audio playback.\n",
    "\n",
    "**Requirements**:\n",
    "```bash\n",
    "pip install music21\n",
    "```\n",
    "\n",
    "**Online ABC Players** (alternative to local MIDI):\n",
    "- https://abcjs.net - Interactive ABC player\n",
    "- https://www.mandolintab.net/abcconverter.php - ABC to MIDI converter"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yb5e3SFyvAh6",
    "outputId": "cb8436b2-d1c7-4636-acc6-53d1b126a8d5"
   },
   "outputs": [],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtinIfR8wWc8",
    "outputId": "ae6b2197-80ad-4b9b-9fe4-ec3e2d205605"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONVERT ABC TO MIDI USING MUSIC21\n",
    "# =============================================================================\n",
    "\n",
    "def abc_to_midi(abc_text, output_path):\n",
    "    \"\"\"\n",
    "    Convert ABC notation to MIDI file using music21.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from music21 import converter, midi\n",
    "        \n",
    "        # Parse ABC notation\n",
    "        score = converter.parse(abc_text, format='abc')\n",
    "        \n",
    "        # Write to MIDI\n",
    "        mf = midi.translate.streamToMidiFile(score)\n",
    "        mf.open(str(output_path), 'wb')\n",
    "        mf.write()\n",
    "        mf.close()\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"music21 not installed. Run: pip install music21\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Conversion error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Convert generated samples to MIDI\n",
    "midi_dir = RESULTS_DIR / 'midi_files'\n",
    "midi_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Converting ABC samples to MIDI...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "conversion_results = []\n",
    "for i, sample in enumerate(all_samples[:10], 1):\n",
    "    midi_path = midi_dir / f'sample_{i:02d}.mid'\n",
    "    success = abc_to_midi(sample, midi_path)\n",
    "    conversion_results.append(success)\n",
    "    status = \"\u2713 Success\" if success else \"\u2717 Failed\"\n",
    "    print(f\"Sample {i}: {status}\")\n",
    "\n",
    "# Summary\n",
    "successful = sum(conversion_results)\n",
    "total = len(conversion_results)\n",
    "print(f\"\\nMIDI Conversion Rate: {successful}/{total} ({100*successful/total:.1f}%)\")\n",
    "\n",
    "if successful > 0:\n",
    "    print(f\"\\nMIDI files saved to: {midi_dir}\")\n",
    "    print(\"Play with: open <file>.mid  (macOS) or use a MIDI player\")\n",
    "else:\n",
    "    print(\"\\nAlternative: Use online ABC players:\")\n",
    "    print(\"  - https://abcjs.net\")\n",
    "    print(\"  - https://www.mandolintab.net/abcconverter.php\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiuCEx2VwWc8"
   },
   "source": [
    "---\n",
    "# 7. Final Summary and Report Data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NX4IjBdkwWc8",
    "outputId": "4b4f8e97-186d-4d18-f582-ef2f0a445df5"
   },
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL PROJECT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\ud83d\udcca DATASET STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total tunes: {len(all_tunes):,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Total tokens: {train_tokens + val_tokens + test_tokens:,}\")\n",
    "print(f\"Sequence length: {SEQ_LENGTH}\")\n",
    "\n",
    "print(\"\\n\ud83e\udd16 TRANSFORMER SCALING RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "for name, res in transformer_results.items():\n",
    "    print(f\"  {name:8s}: {res['params']:>10,} params \u2192 val_loss = {res['val_loss']:.4f}\")\n",
    "\n",
    "if tf_alpha is not None:\n",
    "    print(f\"\\n  Scaling exponent \u03b1 = {tf_alpha:.4f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udd04 LSTM SCALING RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "for name, res in lstm_results.items():\n",
    "    print(f\"  {name:8s}: {res['params']:>10,} params \u2192 val_loss = {res['val_loss']:.4f}\")\n",
    "\n",
    "if lstm_alpha is not None:\n",
    "    print(f\"\\n  Scaling exponent \u03b1 = {lstm_alpha:.4f}\")\n",
    "\n",
    "print(\"\\n\ud83c\udfb5 BEST MODEL PERFORMANCE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Architecture: XL Transformer\")\n",
    "print(f\"Parameters: {best_model.count_params():,}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcdd SAMPLE GENERATION\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Samples generated: {len(all_samples)}\")\n",
    "print(f\"Valid syntax rate: {analysis['validity_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc1 OUTPUT FILES\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "for f in RESULTS_DIR.glob('*'):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_IC_kSwwWc8"
   },
   "source": [
    "---\n",
    "# Appendix A: Example Generated Samples\n",
    "\n",
    "Below are 5 complete generated samples from the best model:"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvxdicgJwWc8",
    "outputId": "cbd1fa0b-660d-4f75-a5cf-ef608e411d23"
   },
   "outputs": [],
   "source": [
    "# Display example samples in formatted boxes\n",
    "for i, sample in enumerate(all_samples[:5], 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAMPLE {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(sample)\n",
    "    is_valid = \"\u2713 Valid\" if validate_abc(sample) else \"\u2717 Invalid\"\n",
    "    print(f\"\\nStatus: {is_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfhaAWKWwWc8"
   },
   "source": [
    "---\n",
    "# Appendix B: Model Architecture Details"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9OctPAdwWc8",
    "outputId": "95ea2a8a-37c9-4bc8-8fdd-b1c6a52e81ef"
   },
   "outputs": [],
   "source": [
    "# Detailed architecture table\n",
    "print(\"TRANSFORMER ARCHITECTURES\")\n",
    "print(\"=\" * 70)\n",
    "tf_arch = []\n",
    "for name, cfg in TRANSFORMER_CONFIGS.items():\n",
    "    model = create_transformer(name)\n",
    "    tf_arch.append({\n",
    "        'Name': name,\n",
    "        'd_model': cfg['d_model'],\n",
    "        'n_heads': cfg['n_heads'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'd_ff': cfg['d_ff'],\n",
    "        'Parameters': f\"{model.count_params():,}\"\n",
    "    })\n",
    "    del model\n",
    "\n",
    "tf_arch_df = pd.DataFrame(tf_arch)\n",
    "print(tf_arch_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLSTM ARCHITECTURES\")\n",
    "print(\"=\" * 70)\n",
    "lstm_arch = []\n",
    "for name, cfg in LSTM_CONFIGS.items():\n",
    "    model = create_lstm(name)\n",
    "    lstm_arch.append({\n",
    "        'Name': name,\n",
    "        'embed_dim': cfg['embed_dim'],\n",
    "        'hidden_dim': cfg['hidden_dim'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'Parameters': f\"{model.count_params():,}\"\n",
    "    })\n",
    "    del model\n",
    "\n",
    "lstm_arch_df = pd.DataFrame(lstm_arch)\n",
    "print(lstm_arch_df.to_string(index=False))\n",
    "\n",
    "# Save architecture tables\n",
    "tf_arch_df.to_csv(RESULTS_DIR / 'transformer_architectures.csv', index=False)\n",
    "lstm_arch_df.to_csv(RESULTS_DIR / 'lstm_architectures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGfawvtdwWc8"
   },
   "source": [
    "---\n",
    "# Appendix C: Training Hyperparameters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfrmqHC3wWc8",
    "outputId": "2a3bab57-d370-4d4f-8c79-d6e66436ab11"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "hyperparams = {\n",
    "    'Hyperparameter': [\n",
    "        'Sequence Length',\n",
    "        'Batch Size',\n",
    "        'Effective Batch Size',\n",
    "        'Gradient Accumulation',\n",
    "        'Learning Rate (scaling)',\n",
    "        'Learning Rate (best model)',\n",
    "        'Weight Decay',\n",
    "        'Optimizer',\n",
    "        'LR Schedule',\n",
    "        'Gradient Clipping',\n",
    "        'Dropout',\n",
    "        'Mixed Precision',\n",
    "        'Epochs (scaling study)',\n",
    "        'Epochs (best model)',\n",
    "        'Data Augmentation',\n",
    "    ],\n",
    "    'Value': [\n",
    "        SEQ_LENGTH,\n",
    "        BATCH_SIZE,\n",
    "        BATCH_SIZE * GRAD_ACCUM_STEPS,\n",
    "        GRAD_ACCUM_STEPS,\n",
    "        '3e-4',\n",
    "        '1e-4',\n",
    "        0.01,\n",
    "        'AdamW (fused)',\n",
    "        'Cosine Annealing',\n",
    "        1.0,\n",
    "        0.1,\n",
    "        'bfloat16' if USE_BF16 else 'float16' if USE_AMP else 'disabled',\n",
    "        1,\n",
    "        BEST_MODEL_EPOCHS,\n",
    "        f'{AUGMENTATION_FACTOR}x',\n",
    "    ]\n",
    "}\n",
    "\n",
    "hp_df = pd.DataFrame(hyperparams)\n",
    "print(\"TRAINING HYPERPARAMETERS (A100 Optimized)\")\n",
    "print(\"=\" * 55)\n",
    "print(hp_df.to_string(index=False))\n",
    "hp_df.to_csv(RESULTS_DIR / 'hyperparameters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXvPvVnZwWc8"
   },
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "1. Kaplan, J., et al. (2020). \"Scaling Laws for Neural Language Models.\" arXiv:2001.08361\n",
    "2. Vaswani, A., et al. (2017). \"Attention Is All You Need.\" NeurIPS 2017\n",
    "3. Karpathy, A. nanoGPT. https://github.com/karpathy/nanoGPT\n",
    "4. ABC Notation Standard. https://abcnotation.com/wiki/abc:standard\n",
    "\n",
    "---\n",
    "\n",
    "**End of Project Notebook**\n",
    "\n",
    "*To run this notebook:*\n",
    "1. Ensure PyTorch is installed\n",
    "2. Run cells sequentially from top to bottom\n",
    "3. Adjust `MAX_BATCHES` based on your compute resources (set to `None` for full training)\n",
    "4. Results will be saved to the `results/` directory"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjMqrYzBwWc8",
    "outputId": "768db72d-c552-463b-86e0-46ff70b95db5"
   },
   "outputs": [],
   "source": [
    "# Save requirements.txt\n",
    "requirements = \"\"\"torch>=2.0.0\n",
    "numpy>=1.21.0\n",
    "pandas>=1.3.0\n",
    "matplotlib>=3.4.0\n",
    "seaborn>=0.11.0\n",
    "scipy>=1.7.0\n",
    "tqdm>=4.62.0\n",
    "music21>=8.0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(PROJECT_DIR / 'requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Requirements saved to requirements.txt\")\n",
    "print(\"\\nInstall with: pip install -r requirements.txt\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\u2705 PROJECT NOTEBOOK COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LaTeX Report for Overleaf\n",
    "\n",
    "Copy the following LaTeX code into Overleaf to generate the PDF report:\n",
    "\n",
    "```latex\n",
    "\\documentclass[11pt,a4paper]{article}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage[T1]{fontenc}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{booktabs}\n",
    "\\usepackage{hyperref}\n",
    "\\usepackage{float}\n",
    "\\usepackage{geometry}\n",
    "\\usepackage{caption}\n",
    "\\usepackage{subcaption}\n",
    "\\usepackage{listings}\n",
    "\\usepackage{xcolor}\n",
    "\n",
    "\\geometry{margin=1in}\n",
    "\\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}\n",
    "\n",
    "\\title{\\textbf{Scaling Laws for Language Models on Symbolic Music Data}}\n",
    "\\author{\n",
    "    ML CS-GY 6923-B Final Project\\\\\n",
    "    New York University Tandon School of Engineering\n",
    "}\n",
    "\\date{December 2025}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\begin{abstract}\n",
    "This project investigates scaling laws for language models trained on symbolic music data represented in ABC notation. We conduct a comprehensive empirical study comparing decoder-only Transformer models and LSTM-based recurrent neural networks across multiple model sizes, ranging from 1M to 100M+ parameters. Using real music data from The Session (53K tune settings) and Nottingham Music Database (1K tunes) with data augmentation, we train models on approximately 100M tokens. Our experiments reveal that Transformers exhibit strong power-law scaling behavior while LSTMs show significantly weaker scaling. The best-performing model generates syntactically valid ABC notation that can be converted to playable MIDI files.\n",
    "\\end{abstract}\n",
    "\n",
    "\\section{Introduction}\n",
    "\n",
    "Recent work by Kaplan et al. (2020) demonstrated that neural language model performance follows predictable power-law relationships with respect to model size, dataset size, and compute budget. These ``scaling laws'' have profound implications for resource allocation and model design decisions.\n",
    "\n",
    "In this project, we extend the study of scaling laws to the domain of \\textbf{symbolic music generation}. Specifically, we train language models on ABC notation---a text-based music representation format widely used for folk and traditional music. Our goals are:\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item Build a complete data preprocessing pipeline for symbolic music\n",
    "    \\item Empirically derive scaling laws for transformer-based language models\n",
    "    \\item Compare transformer vs. RNN scaling behavior on the same task\n",
    "    \\item Train a best model for music generation and evaluate sample quality\n",
    "\\end{enumerate}\n",
    "\n",
    "\\section{Dataset and Preprocessing}\n",
    "\n",
    "\\subsection{Data Sources}\n",
    "\n",
    "We use real music data from two primary sources:\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item \\textbf{The Session} (\\url{https://thesession.org}): 53,282 tune settings covering Irish and folk music\n",
    "    \\item \\textbf{Nottingham Music Database}: 1,037 traditional folk tunes\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{ABC Notation}\n",
    "\n",
    "ABC notation is a human-readable text format for representing musical scores:\n",
    "\n",
    "\\begin{verbatim}\n",
    "X:1\n",
    "T:Example Tune\n",
    "R:reel\n",
    "M:4/4\n",
    "L:1/8\n",
    "K:G\n",
    "G2BG dGBG|c2ec dGBG|G2BG dGBd|egfa gedB|\n",
    "\\end{verbatim}\n",
    "\n",
    "\\subsection{Data Augmentation}\n",
    "\n",
    "To reach the target of 100M training tokens, we apply \\textbf{key transposition augmentation}---shifting all notes by whole tones to create musically valid variations. This is a standard technique in music ML research that preserves melodic structure while increasing dataset diversity.\n",
    "\n",
    "\\subsection{Dataset Statistics}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Dataset Statistics}\n",
    "\\begin{tabular}{lr}\n",
    "\\toprule\n",
    "\\textbf{Metric} & \\textbf{Value} \\\\\n",
    "\\midrule\n",
    "Raw tunes (The Session + Nottingham) & 54,319 \\\\\n",
    "Augmentation factor & 7x \\\\\n",
    "Total tunes after augmentation & 380,233 \\\\\n",
    "Total tokens & $\\sim$100M \\\\\n",
    "Vocabulary size & $\\sim$80 \\\\\n",
    "Train/Val/Test split & 98\\%/1\\%/1\\% \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\\subsection{Tokenization}\n",
    "\n",
    "We employ \\textbf{character-level tokenization} because ABC notation has inherent character-level semantics where each character represents a musical element (note, duration, bar line, etc.).\n",
    "\n",
    "\\section{Model Architectures}\n",
    "\n",
    "\\subsection{Transformer Decoder}\n",
    "\n",
    "Our Transformer implementation follows the GPT architecture:\n",
    "\\begin{itemize}\n",
    "    \\item Causal (autoregressive) self-attention\n",
    "    \\item Pre-layer normalization\n",
    "    \\item Learned positional embeddings\n",
    "    \\item GELU activation functions\n",
    "    \\item Weight tying between embeddings and output projection\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{LSTM Baseline}\n",
    "\n",
    "For comparison, we implement LSTM language models with stacked layers, dropout, and weight tying where applicable.\n",
    "\n",
    "\\subsection{Model Configurations}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Transformer Model Configurations}\n",
    "\\begin{tabular}{lccccc}\n",
    "\\toprule\n",
    "\\textbf{Model} & \\textbf{d\\_model} & \\textbf{n\\_heads} & \\textbf{n\\_layers} & \\textbf{d\\_ff} & \\textbf{Target Params} \\\\\n",
    "\\midrule\n",
    "Tiny & 128 & 4 & 4 & 512 & $\\sim$1M \\\\\n",
    "Small & 256 & 8 & 6 & 1024 & $\\sim$5M \\\\\n",
    "Medium & 512 & 8 & 8 & 2048 & $\\sim$20M \\\\\n",
    "Large & 768 & 12 & 12 & 3072 & $\\sim$50M \\\\\n",
    "XL & 1024 & 16 & 16 & 4096 & $\\sim$100M \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{LSTM Model Configurations (matched parameter counts)}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "\\textbf{Model} & \\textbf{embed\\_dim} & \\textbf{hidden\\_dim} & \\textbf{n\\_layers} & \\textbf{Target Params} \\\\\n",
    "\\midrule\n",
    "Tiny & 256 & 512 & 2 & $\\sim$1M \\\\\n",
    "Small & 384 & 768 & 3 & $\\sim$5M \\\\\n",
    "Medium & 512 & 1024 & 4 & $\\sim$20M \\\\\n",
    "Large & 768 & 1536 & 5 & $\\sim$50M \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\\section{Experiments}\n",
    "\n",
    "\\subsection{Training Setup}\n",
    "\n",
    "All models were trained with consistent hyperparameters:\n",
    "\\begin{itemize}\n",
    "    \\item \\textbf{Optimizer}: AdamW ($\\beta_1=0.9$, $\\beta_2=0.999$, weight decay=0.01)\n",
    "    \\item \\textbf{Learning rate}: $3 \\times 10^{-4}$ with cosine annealing\n",
    "    \\item \\textbf{Batch size}: 64\n",
    "    \\item \\textbf{Sequence length}: 256 tokens\n",
    "    \\item \\textbf{Gradient clipping}: max norm = 1.0\n",
    "    \\item \\textbf{Training}: Exactly 1 epoch per model for scaling comparison\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{Scaling Study Protocol}\n",
    "\n",
    "Each model was trained for exactly 1 epoch on the same $\\sim$100M token training set. We record validation loss, training time, and GPU memory usage. The scaling exponent $\\alpha$ is fit using the power law:\n",
    "\n",
    "\\begin{equation}\n",
    "L = a \\cdot N^{-\\alpha} + c\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the parameter count and $L$ is the validation loss.\n",
    "\n",
    "\\section{Results}\n",
    "\n",
    "\\subsection{Transformer Scaling}\n",
    "\n",
    "[Insert results table from notebook output]\n",
    "\n",
    "The fitted scaling exponent $\\alpha$ indicates the rate at which loss decreases with model size. Higher $\\alpha$ means better scaling efficiency.\n",
    "\n",
    "\\subsection{LSTM Scaling}\n",
    "\n",
    "[Insert results table from notebook output]\n",
    "\n",
    "\\subsection{Architecture Comparison}\n",
    "\n",
    "Key findings:\n",
    "\\begin{enumerate}\n",
    "    \\item Transformers exhibit significantly steeper scaling curves than LSTMs\n",
    "    \\item LSTMs show diminishing returns at larger model sizes\n",
    "    \\item Transformers achieve lower loss at equivalent parameter counts\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Best Model Performance}\n",
    "\n",
    "[Insert best model metrics: perplexity, sample validity rate, MIDI conversion success]\n",
    "\n",
    "\\section{Sample Generation}\n",
    "\n",
    "We generate music samples using nucleus sampling with temperature $T=0.8$ and top-$k=40$.\n",
    "\n",
    "\\subsection{Quantitative Evaluation}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item \\textbf{Test Perplexity}: [from notebook]\n",
    "    \\item \\textbf{Syntactic Validity Rate}: Percentage of samples with valid ABC structure\n",
    "    \\item \\textbf{MIDI Conversion Rate}: Percentage successfully converted via music21\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{Qualitative Analysis}\n",
    "\n",
    "Generated samples demonstrate:\n",
    "\\begin{itemize}\n",
    "    \\item Correct ABC header structure (X:, T:, M:, K:)\n",
    "    \\item Valid note sequences with proper bar lines\n",
    "    \\item Rhythmic patterns consistent with specified meter\n",
    "    \\item Key-appropriate note choices\n",
    "\\end{itemize}\n",
    "\n",
    "\\section{Discussion}\n",
    "\n",
    "\\subsection{Key Insights}\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item \\textbf{Scaling Laws Transfer}: Power-law scaling observed in NLP transfers to symbolic music\n",
    "    \\item \\textbf{Transformer Superiority}: Attention mechanisms scale more efficiently than recurrence\n",
    "    \\item \\textbf{Data Quality Matters}: Real music data produces more coherent generations than synthetic\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Limitations}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item Dataset limited to folk/traditional music genres\n",
    "    \\item Character-level tokenization may miss higher-level musical structure\n",
    "    \\item Evaluation is primarily syntactic; musical quality requires human evaluation\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{Future Work}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item Use full Lakh MIDI dataset with MIDI-to-ABC conversion\n",
    "    \\item Implement music-aware tokenization (note-level or bar-level)\n",
    "    \\item Conduct human evaluation studies for musical quality\n",
    "    \\item Explore longer context windows for multi-part compositions\n",
    "\\end{itemize}\n",
    "\n",
    "\\section{Conclusion}\n",
    "\n",
    "This project demonstrates that scaling laws for language models extend to symbolic music generation. Transformer models exhibit strong power-law scaling on ABC notation data, significantly outperforming LSTM baselines. Our best model generates syntactically valid and musically plausible ABC notation that can be converted to playable MIDI files.\n",
    "\n",
    "\\section*{References}\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item Kaplan, J., et al. (2020). ``Scaling Laws for Neural Language Models.'' \\textit{arXiv:2001.08361}\n",
    "    \\item Vaswani, A., et al. (2017). ``Attention Is All You Need.'' \\textit{NeurIPS 2017}\n",
    "    \\item Karpathy, A. nanoGPT. \\url{https://github.com/karpathy/nanoGPT}\n",
    "    \\item The Session. \\url{https://thesession.org}\n",
    "    \\item ABC Notation Standard. \\url{https://abcnotation.com/wiki/abc:standard}\n",
    "\\end{enumerate}\n",
    "\n",
    "\\appendix\n",
    "\n",
    "\\section{Code Repository Structure}\n",
    "\n",
    "\\begin{verbatim}\n",
    "final_project/\n",
    "\u251c\u2500\u2500 00_ml_cs_gy_6923_b_final_project.ipynb  # Main notebook\n",
    "\u251c\u2500\u2500 music_data/                              # Downloaded data\n",
    "\u2502   \u251c\u2500\u2500 thesession_data.json\n",
    "\u2502   \u2514\u2500\u2500 nottingham-dataset-master/\n",
    "\u251c\u2500\u2500 models/                                  # Saved checkpoints\n",
    "\u251c\u2500\u2500 results/                                 # Plots and outputs\n",
    "\u2502   \u251c\u2500\u2500 generated_samples/\n",
    "\u2502   \u2514\u2500\u2500 midi_files/\n",
    "\u251c\u2500\u2500 requirements.txt\n",
    "\u2514\u2500\u2500 README.md\n",
    "\\end{verbatim}\n",
    "\n",
    "\\section{Example Generated Samples}\n",
    "\n",
    "[Include 5 best samples from notebook output]\n",
    "\n",
    "\\end{document}\n",
    "```"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}