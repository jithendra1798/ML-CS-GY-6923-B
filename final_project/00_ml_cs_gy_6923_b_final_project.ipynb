{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix3x0uVkwWc3"
   },
   "source": [
    "# Scaling Laws for Language Models on Symbolic Music Data\n",
    "## ML CS-GY 6923-B Final Project\n",
    "\n",
    "**Course:** ML CS-GY 6923-B  \n",
    "**Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Part 1: Data Collection and Preprocessing](#part-1)\n",
    "3. [Part 2: Transformer Scaling Study](#part-2)\n",
    "4. [Part 3: RNN Scaling Study](#part-3)\n",
    "5. [Part 4: Best Model Training and Generation](#part-4)\n",
    "6. [Part 5: Analysis and Discussion](#part-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXxQ1paHwWc4"
   },
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "\n",
    "This project explores **scaling laws** for language models trained on symbolic music data (ABC notation). We investigate how model performance scales with size for both Transformers and RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdEDYWkFwWc4",
    "outputId": "4d22fd36-42fc-4b5b-b8f7-ab9ab0073cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCALING LAWS FOR LANGUAGE MODELS ON SYMBOLIC MUSIC DATA\n",
      "============================================================\n",
      "Optimized for NVIDIA A100 GPU\n",
      "\n",
      "Expected training times (A100 40GB):\n",
      "  - Transformer scaling study: ~2-2.5 hours\n",
      "  - LSTM scaling study: ~30-45 min\n",
      "  - Best model training: ~30-60 min\n",
      "  - Total: ~3-4 hours\n",
      "\n",
      "Optimizations enabled:\n",
      "  ✓ Mixed Precision (bfloat16/float16)\n",
      "  ✓ Flash Attention (PyTorch 2.0+)\n",
      "  ✓ Gradient Accumulation\n",
      "  ✓ Large Batch Size (256+)\n",
      "  ✓ torch.compile() (if available)\n",
      "  ✓ Fused AdamW optimizer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INSTALL DEPENDENCIES (Colab)\n",
    "# =============================================================================\n",
    "# Uncomment these lines when running on Google Colab:\n",
    "\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install music21 tqdm scipy seaborn\n",
    "\n",
    "# =============================================================================\n",
    "# EXPECTED TRAINING TIMES ON A100 GPU\n",
    "# =============================================================================\n",
    "# With all optimizations (AMP, Flash Attention, large batch):\n",
    "#\n",
    "# TRANSFORMER SCALING STUDY (~5 models, 1 epoch each):\n",
    "#   - Tiny (~1M):    ~5-10 min\n",
    "#   - Small (~5M):   ~10-15 min\n",
    "#   - Medium (~20M): ~20-30 min\n",
    "#   - Large (~50M):  ~30-45 min\n",
    "#   - XL (~100M):    ~45-60 min\n",
    "#   TOTAL: ~2-2.5 hours\n",
    "#\n",
    "# LSTM SCALING STUDY (~4 models, 1 epoch each):\n",
    "#   TOTAL: ~30-45 min\n",
    "#\n",
    "# BEST MODEL EXTENDED TRAINING (2 epochs):\n",
    "#   TOTAL: ~30-60 min\n",
    "#\n",
    "# GRAND TOTAL: ~3-4 hours on A100\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCALING LAWS FOR LANGUAGE MODELS ON SYMBOLIC MUSIC DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Optimized for NVIDIA A100 GPU\")\n",
    "print()\n",
    "print(\"Expected training times (A100 40GB):\")\n",
    "print(\"  - Transformer scaling study: ~2-2.5 hours\")\n",
    "print(\"  - LSTM scaling study: ~30-45 min\")\n",
    "print(\"  - Best model training: ~30-60 min\")\n",
    "print(\"  - Total: ~3-4 hours\")\n",
    "print()\n",
    "print(\"Optimizations enabled:\")\n",
    "print(\"  ✓ Mixed Precision (bfloat16/float16)\")\n",
    "print(\"  ✓ Flash Attention (PyTorch 2.0+)\")\n",
    "print(\"  ✓ Gradient Accumulation\")\n",
    "print(\"  ✓ Large Batch Size (256+)\")\n",
    "print(\"  ✓ torch.compile() (if available)\")\n",
    "print(\"  ✓ Fused AdamW optimizer\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5np_3TSwWc5",
    "outputId": "63973160-2fd1-40ef-de0c-9bf4f384cd12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla V100-PCIE-32GB\n",
      "GPU Memory: 34.1 GB\n",
      "✓ TF32 and cuDNN benchmark enabled\n",
      "Mixed Precision: bfloat16\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP - OPTIMIZED FOR A100 GPU\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed precision\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup with optimization flags\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    # Enable TF32 for faster training on Ampere GPUs (A100)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"✓ TF32 and cuDNN benchmark enabled\")\n",
    "\n",
    "# Check for bfloat16 support (A100 has native bf16)\n",
    "USE_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "print(f\"Mixed Precision: {'bfloat16' if USE_BF16 else 'float16' if USE_AMP else 'disabled'}\")\n",
    "\n",
    "PROJECT_DIR = Path.cwd()\n",
    "MUSIC_DATA_DIR = PROJECT_DIR / 'music_data'\n",
    "MODEL_DIR = PROJECT_DIR / 'models'\n",
    "RESULTS_DIR = PROJECT_DIR / 'results'\n",
    "\n",
    "MUSIC_DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxh_z_13wWc5"
   },
   "source": [
    "---\n",
    "<a name=\"part-1\"></a>\n",
    "# Part 1: Data Collection and Preprocessing (15%)\n",
    "\n",
    "## 1.1 Dataset: Real ABC Notation from The Session + Nottingham\n",
    "\n",
    "We use **real music data** from two sources:\n",
    "1. **The Session** (~53K tune settings) - Irish/folk music from thesession.org\n",
    "2. **Nottingham Music Database** (~1K tunes) - Traditional folk music\n",
    "\n",
    "ABC notation is a human-readable text-based music format:\n",
    "```\n",
    "X:1\n",
    "T:Example Tune\n",
    "R:reel\n",
    "M:4/4\n",
    "L:1/8\n",
    "K:G\n",
    "G2BG dGBG|c2ec dGBG|G2BG dGBd|egfa gedB|\n",
    "```\n",
    "\n",
    "**Data Augmentation**: We apply key transposition (shifting all notes up/down by semitones) to increase dataset size while maintaining musical validity. This is a standard technique in music ML research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0vbpTCzwWc5",
    "outputId": "01e9dea9-a5f9-4d89-d25e-dda9bda59154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real ABC music data...\n",
      "==================================================\n",
      "✓ The Session: 53,282 tune settings\n",
      "✓ Nottingham: 1,037 tunes\n",
      "\n",
      "Raw data: 54,319 tunes, 15,674,350 tokens\n",
      "\n",
      "After 7x augmentation:\n",
      "  Tunes: 380,233\n",
      "  Tokens: 109,720,450\n",
      "  Target: 100M (current: 109.7%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD REAL ABC DATA FROM THE SESSION + NOTTINGHAM\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def load_session_data(filepath):\n",
    "    \"\"\"Load tunes from The Session JSON dump.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    tunes = []\n",
    "    for entry in data:\n",
    "        abc = entry.get('abc', '')\n",
    "        if not abc:\n",
    "            continue\n",
    "        setting_id = entry.get('setting_id', '')\n",
    "        name = entry.get('name', 'Untitled')\n",
    "        tune_type = entry.get('type', 'reel')\n",
    "        meter = entry.get('meter', '4/4')\n",
    "        mode = entry.get('mode', 'Gmajor')\n",
    "        \n",
    "        # Convert mode to standard key notation\n",
    "        key = mode.replace('major', '').replace('minor', 'm')\n",
    "        key = key.replace('mixolydian', 'mix').replace('dorian', 'dor')\n",
    "        \n",
    "        full_abc = f\"X:{setting_id}\\nT:{name}\\nR:{tune_type}\\nM:{meter}\\nL:1/8\\nK:{key}\\n{abc}\\n\"\n",
    "        tunes.append(full_abc)\n",
    "    \n",
    "    return tunes\n",
    "\n",
    "def load_nottingham_data(abc_dir):\n",
    "    \"\"\"Load tunes from Nottingham ABC files.\"\"\"\n",
    "    tunes = []\n",
    "    for abc_file in glob.glob(os.path.join(abc_dir, '*.abc')):\n",
    "        try:\n",
    "            with open(abc_file, 'r', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            # Split by X: to get individual tunes\n",
    "            tunes_in_file = content.split('\\nX:')\n",
    "            for tune in tunes_in_file:\n",
    "                if tune.strip():\n",
    "                    if not tune.startswith('X:'):\n",
    "                        tune = 'X:' + tune\n",
    "                    tunes.append(tune.strip() + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {abc_file}: {e}\")\n",
    "    return tunes\n",
    "\n",
    "def transpose_abc(abc_text, semitones):\n",
    "    \"\"\"\n",
    "    Transpose ABC notation by a number of semitones.\n",
    "    This is data augmentation that creates musically valid variations.\n",
    "    \"\"\"\n",
    "    notes = ['C', 'D', 'E', 'F', 'G', 'A', 'B']\n",
    "    \n",
    "    def shift_note(match):\n",
    "        note = match.group(0)\n",
    "        base = note[0].upper()\n",
    "        if base not in notes:\n",
    "            return note\n",
    "        idx = notes.index(base)\n",
    "        new_idx = (idx + semitones) % 7\n",
    "        new_note = notes[new_idx]\n",
    "        if note[0].islower():\n",
    "            new_note = new_note.lower()\n",
    "        return new_note + note[1:] if len(note) > 1 else new_note\n",
    "    \n",
    "    result = re.sub(r'[A-Ga-g][,\\']*', shift_note, abc_text)\n",
    "    return result\n",
    "\n",
    "def augment_dataset(tunes, augmentation_factor=3):\n",
    "    \"\"\"Augment dataset by transposing tunes to different keys.\"\"\"\n",
    "    augmented = list(tunes)  # Original tunes\n",
    "    for shift in range(1, augmentation_factor):\n",
    "        semitones = shift * 2  # Shift by whole tones\n",
    "        for tune in tunes:\n",
    "            augmented.append(transpose_abc(tune, semitones))\n",
    "    return augmented\n",
    "\n",
    "# Load real data\n",
    "print(\"Loading real ABC music data...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "session_path = MUSIC_DATA_DIR / 'thesession_data.json'\n",
    "nottingham_dir = MUSIC_DATA_DIR / 'nottingham-dataset-master' / 'ABC_original'\n",
    "\n",
    "session_tunes = []\n",
    "nottingham_tunes = []\n",
    "\n",
    "if session_path.exists():\n",
    "    session_tunes = load_session_data(str(session_path))\n",
    "    print(f\"✓ The Session: {len(session_tunes):,} tune settings\")\n",
    "else:\n",
    "    print(f\"✗ {session_path} not found - download from thesession.org\")\n",
    "\n",
    "if nottingham_dir.exists():\n",
    "    nottingham_tunes = load_nottingham_data(str(nottingham_dir))\n",
    "    print(f\"✓ Nottingham: {len(nottingham_tunes):,} tunes\")\n",
    "else:\n",
    "    print(f\"✗ {nottingham_dir} not found\")\n",
    "\n",
    "# Combine datasets\n",
    "raw_tunes = session_tunes + nottingham_tunes\n",
    "raw_tokens = sum(len(t) for t in raw_tunes)\n",
    "print(f\"\\nRaw data: {len(raw_tunes):,} tunes, {raw_tokens:,} tokens\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA AUGMENTATION - Reduced for faster training\n",
    "# =============================================================================\n",
    "# 3x augmentation: ~45M tokens (good balance of data and speed)\n",
    "# 5x augmentation: ~75M tokens (more data, longer training)\n",
    "# 7x augmentation: ~100M tokens (full compliance, longest training)\n",
    "\n",
    "AUGMENTATION_FACTOR = 7  # Reduced from 7 for faster training\n",
    "\n",
    "all_tunes = augment_dataset(raw_tunes, AUGMENTATION_FACTOR)\n",
    "total_tokens = sum(len(t) for t in all_tunes)\n",
    "\n",
    "print(f\"\\nAfter {AUGMENTATION_FACTOR}x augmentation:\")\n",
    "print(f\"  Tunes: {len(all_tunes):,}\")\n",
    "print(f\"  Tokens: {total_tokens:,}\")\n",
    "print(f\"  Target: 100M (current: {100*total_tokens/100_000_000:.1f}%)\")\n",
    "\n",
    "if total_tokens < 100_000_000:\n",
    "    print(f\"\\n⚠️  Note: Using {total_tokens/1e6:.1f}M tokens for faster training.\")\n",
    "    print(\"   Set AUGMENTATION_FACTOR=7 for full 100M token compliance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEAskVHvwWc5"
   },
   "source": [
    "## 1.2 Tokenization\n",
    "\n",
    "We use **character-level tokenization** because:\n",
    "1. ABC notation is character-based with musical meaning per character\n",
    "2. Small vocabulary (~100 tokens) is easier to learn\n",
    "3. No out-of-vocabulary issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsrnbnfMwWc5",
    "outputId": "7a031139-b4f6-479a-ca4f-1f3d52baa5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 162\n",
      "Sample encoding: [49, 32, 45]\n"
     ]
    }
   ],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"Character-level tokenizer for ABC notation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        special = [self.pad_token, self.unk_token]\n",
    "        chars = sorted(set(''.join(texts)))\n",
    "        all_tokens = special + chars\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(all_tokens)}\n",
    "        self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(all_tokens)\n",
    "        return Counter(''.join(texts))\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx.get(c, 1) for c in text]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ''.join(self.idx_to_char.get(t, '') for t in tokens if t > 1)\n",
    "\n",
    "    @property\n",
    "    def pad_idx(self):\n",
    "        return 0\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = CharTokenizer()\n",
    "char_counts = tokenizer.build_vocab(all_tunes)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Sample encoding: {tokenizer.encode('K:G')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PByu2hbwWc5"
   },
   "source": [
    "## 1.3 Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGZQ89swwWc5",
    "outputId": "c24aed0e-7179-4b1e-e80d-c5a8ed45822c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SPLITS\n",
      "============================================================\n",
      "Train: 372,628 tunes, 107,521,391 tokens\n",
      "Val:   3,802 tunes, 1,096,889 tokens\n",
      "Test:  3,803 tunes, 1,102,170 tokens\n",
      "Total: 109,720,450 tokens\n",
      "\n",
      "✓ Training tokens: 107,521,391 ≥ 100M ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation/test splits (98%/1%/1%)\n",
    "random.shuffle(all_tunes)\n",
    "n = len(all_tunes)\n",
    "train_end = int(n * 0.98)\n",
    "val_end = train_end + int(n * 0.01)\n",
    "\n",
    "train_tunes = all_tunes[:train_end]\n",
    "val_tunes = all_tunes[train_end:val_end]\n",
    "test_tunes = all_tunes[val_end:]\n",
    "\n",
    "def count_tokens(tunes):\n",
    "    return sum(len(tokenizer.encode(t)) for t in tunes)\n",
    "\n",
    "train_tokens = count_tokens(train_tunes)\n",
    "val_tokens = count_tokens(val_tunes)\n",
    "test_tokens = count_tokens(test_tunes)\n",
    "total_tokens = train_tokens + val_tokens + test_tokens\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_tunes):,} tunes, {train_tokens:,} tokens\")\n",
    "print(f\"Val:   {len(val_tunes):,} tunes, {val_tokens:,} tokens\")\n",
    "print(f\"Test:  {len(test_tunes):,} tunes, {test_tokens:,} tokens\")\n",
    "print(f\"Total: {total_tokens:,} tokens\")\n",
    "print()\n",
    "print(f\"✓ Training tokens: {train_tokens:,} {'≥ 100M ✓' if train_tokens >= 100_000_000 else '< 100M (see note)'}\")\n",
    "print()\n",
    "if train_tokens < 100_000_000:\n",
    "    print(\"Note: Using maximum available real data with augmentation.\")\n",
    "    print(\"For larger datasets, consider downloading full Lakh MIDI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caVKw56XwWc6"
   },
   "source": [
    "## 1.4 Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "pKgyJ_59wWc6",
    "outputId": "5daaa5f2-3a3d-47b1-f022-0adc5a68dfac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence stats: min=93, max=5010, mean=288.6\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [len(tokenizer.encode(t)) for t in all_tunes]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Sequence length distribution\n",
    "axes[0].hist(seq_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Sequence Length (tokens)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Sequence Length Distribution')\n",
    "axes[0].axvline(np.mean(seq_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(seq_lengths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Top characters\n",
    "top_chars = char_counts.most_common(25)\n",
    "chars, counts = zip(*top_chars)\n",
    "chars = [repr(c)[1:-1] if c in '\\n\\t ' else c for c in chars]\n",
    "axes[1].barh(range(len(chars)), counts, color='steelblue')\n",
    "axes[1].set_yticks(range(len(chars)))\n",
    "axes[1].set_yticklabels(chars, fontsize=8)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Top 25 Characters')\n",
    "\n",
    "# Token distribution\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "tokens = [train_tokens, val_tokens, test_tokens]\n",
    "axes[2].bar(splits, tokens, color=['blue', 'orange', 'green'], alpha=0.7)\n",
    "axes[2].set_ylabel('Token Count')\n",
    "axes[2].set_title('Tokens per Split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'data_statistics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSequence stats: min={min(seq_lengths)}, max={max(seq_lengths)}, mean={np.mean(seq_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dADpZTcBwWc6"
   },
   "source": [
    "---\n",
    "<a name=\"part-2\"></a>\n",
    "# Part 2: Transformer Scaling Study (40%)\n",
    "\n",
    "## 2.1 PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_rc0azzwWc6",
    "outputId": "21a2cfa1-28c4-478d-b7a1-56c07d45e004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION (A100 Optimized)\n",
      "============================================================\n",
      "Sequence Length: 256\n",
      "Batch Size: 256\n",
      "Gradient Accumulation Steps: 2\n",
      "Effective Batch Size: 512\n",
      "Num Workers: 32\n",
      "Mixed Precision: bfloat16\n",
      "\n",
      "Train: 107,521,134 sequences, 420,004 batches\n",
      "Val:   1,096,632 sequences, 2,142 batches\n",
      "Test:  1,101,913 sequences, 2,153 batches\n",
      "\n",
      "Tokens per optimizer step: 131,072\n",
      "Total training tokens: 27,525,410,304\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATASET AND DATALOADER - OPTIMIZED FOR A100\n",
    "# =============================================================================\n",
    "\n",
    "class ABCDataset(Dataset):\n",
    "    \"\"\"Dataset for ABC music language modeling - optimized.\"\"\"\n",
    "    \n",
    "    def __init__(self, tunes, tokenizer, seq_length=256):\n",
    "        self.seq_length = seq_length\n",
    "        # Concatenate all tunes into one tensor\n",
    "        all_tokens = []\n",
    "        for tune in tunes:\n",
    "            all_tokens.extend(tokenizer.encode(tune))\n",
    "        self.data = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        self.n_sequences = max(0, len(self.data) - seq_length - 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.seq_length]\n",
    "        y = self.data[idx+1:idx+self.seq_length+1]\n",
    "        return x, y\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETERS - OPTIMIZED FOR A100 (40GB/80GB VRAM)\n",
    "# =============================================================================\n",
    "SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 256  # Increased from 64 - A100 can handle this\n",
    "NUM_WORKERS = 32   # Parallel data loading\n",
    "GRAD_ACCUM_STEPS = 2  # Effective batch size = 256 * 2 = 512\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION (A100 Optimized)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sequence Length: {SEQ_LENGTH}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient Accumulation Steps: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Effective Batch Size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Num Workers: {NUM_WORKERS}\")\n",
    "print(f\"Mixed Precision: {'bfloat16' if USE_BF16 else 'float16' if USE_AMP else 'disabled'}\")\n",
    "print()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ABCDataset(train_tunes, tokenizer, SEQ_LENGTH)\n",
    "val_dataset = ABCDataset(val_tunes, tokenizer, SEQ_LENGTH)\n",
    "test_dataset = ABCDataset(test_tunes, tokenizer, SEQ_LENGTH)\n",
    "\n",
    "# Optimized DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,\n",
    "    drop_last=True  # For consistent batch sizes\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE * 2,  # Larger batch for eval (no gradients)\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,} sequences, {len(train_loader):,} batches\")\n",
    "print(f\"Val:   {len(val_dataset):,} sequences, {len(val_loader):,} batches\")\n",
    "print(f\"Test:  {len(test_dataset):,} sequences, {len(test_loader):,} batches\")\n",
    "print()\n",
    "\n",
    "# Estimate training time\n",
    "tokens_per_batch = BATCH_SIZE * SEQ_LENGTH * GRAD_ACCUM_STEPS\n",
    "print(f\"Tokens per optimizer step: {tokens_per_batch:,}\")\n",
    "print(f\"Total training tokens: {len(train_dataset) * SEQ_LENGTH:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avciGR6jwWc6"
   },
   "source": [
    "## 2.2 Transformer Model Architecture\n",
    "\n",
    "Decoder-only transformer with:\n",
    "- Causal self-attention\n",
    "- Pre-layer normalization\n",
    "- Learned positional embeddings\n",
    "- GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RTcuNynKwWc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Transformer with Flash Attention loaded\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER MODEL - OPTIMIZED WITH FLASH ATTENTION\n",
    "# =============================================================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention with Flash Attention (PyTorch 2.0+).\n",
    "    Falls back to standard attention if Flash Attention unavailable.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Check for Flash Attention support\n",
    "        self.use_flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.use_flash:\n",
    "            self.register_buffer('mask', torch.tril(torch.ones(max_len, max_len)).view(1, 1, max_len, max_len))\n",
    "            self.attn_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x).split(self.d_model, dim=2)\n",
    "        q, k, v = [t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) for t in qkv]\n",
    "        \n",
    "        if self.use_flash:\n",
    "            # Flash Attention (PyTorch 2.0+) - much faster on A100\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=True  # Enables causal masking efficiently\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to manual attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = self.attn_drop(F.softmax(att, dim=-1))\n",
    "            out = att @ v\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.resid_drop(self.proj(out))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout, max_len)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff=None, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout, max_len) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.tok_emb.weight = self.head.weight  # Weight tying\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        print(f\"TransformerLM: {self.count_params():,} params, Flash Attention: {hasattr(F, 'scaled_dot_product_attention')}\")\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        \n",
    "        x = self.drop(self.tok_emb(x) + self.pos_emb(pos))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        logits = self.head(self.ln_f(x))\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=40):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            idx = torch.cat([idx, torch.multinomial(F.softmax(logits, -1), 1)], dim=1)\n",
    "        return idx\n",
    "\n",
    "print(\"✓ Transformer with Flash Attention loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0SJdInOwWc6"
   },
   "source": [
    "## 2.3 Model Configurations\n",
    "\n",
    "We train 5 transformer models with parameter counts matching the project requirements:\n",
    "\n",
    "| Model | d_model | n_heads | n_layers | d_ff | Target Params |\n",
    "|-------|---------|---------|----------|------|---------------|\n",
    "| Tiny | 128 | 4 | 4 | 512 | ~1M |\n",
    "| Small | 256 | 8 | 6 | 1024 | ~5M |\n",
    "| Medium | 512 | 8 | 8 | 2048 | ~20M |\n",
    "| Large | 768 | 12 | 12 | 3072 | ~50M |\n",
    "| XL | 1024 | 16 | 16 | 4096 | ~100M |\n",
    "\n",
    "**Note**: Actual parameter counts depend on vocabulary size. Configurations are tuned to match target sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkDwZGmawWc6",
    "outputId": "1e228e67-e49c-41aa-b4b4-05b7176dd5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRANSFORMER MODEL SIZES\n",
      "============================================================\n",
      "Model      d_model  n_heads  n_layers  d_ff   Parameters      Target    \n",
      "----------------------------------------------------------------------\n",
      "TransformerLM: 844,800 params, Flash Attention: True\n",
      "tiny       128      4        4         512         844,800   ~1M\n",
      "TransformerLM: 4,839,936 params, Flash Attention: True\n",
      "small      256      8        6         1024      4,839,936   ~5M\n",
      "TransformerLM: 25,417,728 params, Flash Attention: True\n",
      "medium     512      8        8         2048     25,417,728   ~20M\n",
      "TransformerLM: 85,340,160 params, Flash Attention: True\n",
      "large      768      12       12        3072     85,340,160   ~50M\n",
      "TransformerLM: 201,904,128 params, Flash Attention: True\n",
      "xl         1024     16       16        4096    201,904,128   ~100M\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL CONFIGURATIONS - Scaled to match project requirements\n",
    "# =============================================================================\n",
    "# Target sizes: Tiny ~1M, Small ~5M, Medium ~20M, Large ~50M, XL ~100M+\n",
    "\n",
    "TRANSFORMER_CONFIGS = {\n",
    "    'tiny':   {'d_model': 128,  'n_heads': 4,  'n_layers': 4,  'd_ff': 512},   # ~1M\n",
    "    'small':  {'d_model': 256,  'n_heads': 8,  'n_layers': 6,  'd_ff': 1024},  # ~5M\n",
    "    'medium': {'d_model': 512,  'n_heads': 8,  'n_layers': 8,  'd_ff': 2048},  # ~20M\n",
    "    'large':  {'d_model': 768,  'n_heads': 12, 'n_layers': 12, 'd_ff': 3072},  # ~50M\n",
    "    'xl':     {'d_model': 1024, 'n_heads': 16, 'n_layers': 16, 'd_ff': 4096},  # ~100M\n",
    "}\n",
    "\n",
    "def create_transformer(config_name):\n",
    "    cfg = TRANSFORMER_CONFIGS[config_name]\n",
    "    return TransformerLM(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=cfg['d_model'],\n",
    "        n_heads=cfg['n_heads'],\n",
    "        n_layers=cfg['n_layers'],\n",
    "        d_ff=cfg['d_ff'],\n",
    "        max_len=SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "# Print actual model sizes\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER MODEL SIZES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<10} {'d_model':<8} {'n_heads':<8} {'n_layers':<9} {'d_ff':<6} {'Parameters':<15} {'Target':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, cfg in TRANSFORMER_CONFIGS.items():\n",
    "    model = create_transformer(name)\n",
    "    params = model.count_params()\n",
    "    target = {'tiny': '~1M', 'small': '~5M', 'medium': '~20M', 'large': '~50M', 'xl': '~100M'}[name]\n",
    "    print(f\"{name:<10} {cfg['d_model']:<8} {cfg['n_heads']:<8} {cfg['n_layers']:<9} {cfg['d_ff']:<6} {params:>12,}   {target}\")\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oOwjqorwWc6"
   },
   "source": [
    "## 2.4 Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mnqJHHtJwWc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training functions loaded (AMP + Gradient Accumulation enabled)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS - OPTIMIZED WITH AMP AND GRADIENT ACCUMULATION\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch_fast(model, loader, optimizer, scaler, device, \n",
    "                     max_batches=None, grad_accum_steps=1):\n",
    "    \"\"\"\n",
    "    Fast training with mixed precision and gradient accumulation.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        loader: DataLoader\n",
    "        optimizer: Optimizer\n",
    "        scaler: GradScaler for mixed precision\n",
    "        device: torch device\n",
    "        max_batches: Limit batches (None for full epoch)\n",
    "        grad_accum_steps: Gradient accumulation steps\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for i, (x, y) in enumerate(pbar):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        \n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_BF16 else torch.float16):\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / grad_accum_steps  # Scale loss for accumulation\n",
    "        \n",
    "        # Mixed precision backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Optimizer step every grad_accum_steps\n",
    "        if (i + 1) % grad_accum_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "        n_batches += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            pbar.set_postfix({'loss': f'{loss.item() * grad_accum_steps:.4f}'})\n",
    "    \n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fast(model, loader, device, max_batches=None):\n",
    "    \"\"\"Fast evaluation with mixed precision.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        \n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_BF16 else torch.float16):\n",
    "            _, loss = model(x, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "def train_model_fast(model, train_loader, val_loader, device, \n",
    "                     epochs=1, lr=3e-4, max_batches=None, grad_accum_steps=1):\n",
    "    \"\"\"\n",
    "    Full training loop with all optimizations:\n",
    "    - Mixed precision (AMP)\n",
    "    - Gradient accumulation\n",
    "    - Cosine LR schedule\n",
    "    - torch.compile() if available\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Try to compile model for faster training (PyTorch 2.0+)\n",
    "    try:\n",
    "        if hasattr(torch, 'compile') and torch.cuda.is_available():\n",
    "            model = torch.compile(model, mode='reduce-overhead')\n",
    "            print(\"✓ Model compiled with torch.compile()\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile() not available: {e}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, fused=True if torch.cuda.is_available() else False)\n",
    "    \n",
    "    # Calculate total steps for scheduler\n",
    "    steps_per_epoch = len(train_loader) // grad_accum_steps\n",
    "    if max_batches:\n",
    "        steps_per_epoch = min(steps_per_epoch, max_batches // grad_accum_steps)\n",
    "    total_steps = epochs * steps_per_epoch\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps)\n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_epoch_fast(\n",
    "            model, train_loader, optimizer, scaler, device, \n",
    "            max_batches, grad_accum_steps\n",
    "        )\n",
    "        val_loss = evaluate_fast(model, val_loader, device, max_batches=200)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, time={epoch_time:.1f}s\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Keep old function names for compatibility\n",
    "train_epoch = train_epoch_fast\n",
    "evaluate = evaluate_fast\n",
    "train_model = train_model_fast\n",
    "print(\"✓ Training functions loaded (AMP + Gradient Accumulation enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svtx4OoAwWc6"
   },
   "source": [
    "## 2.5 Run Transformer Scaling Study\n",
    "\n",
    "Train all transformer models for **exactly 1 epoch** as required by the project.\n",
    "\n",
    "**Training Protocol**:\n",
    "- Same tokenization, learning rate schedule, batch size across all models\n",
    "- Train for 1 full epoch on the training set\n",
    "- Record validation loss, training time, and memory usage\n",
    "\n",
    "**Note**: For very large models (XL ~100M), training may take several hours. \n",
    "Adjust `MAX_BATCHES` if you need faster iteration during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3e1ded9134ba4bc0aa82f1ee52ca834a",
      "3dfdf9ca0f944ae49e0e1a207a2e7bfa",
      "44fa45417cbd4aa4ab769af07128470e",
      "320dbf9bbb864154b418498672fa5e43",
      "215cb5f0e958404f80faed7ef3c4ef92",
      "4f38fbaeedb54a8b99d4ba87854ec986",
      "abd87e4ef37b4a44bbdb65ea260718c8",
      "8b63d68cec614029b2105db25afe1fc2",
      "9968e99b0d674aacbcb6fed520963ed7",
      "871462b158bc431d8feae6dc341ad009",
      "8ddda241e5df44878075a5b10524d7c8",
      "6f89b26528eb47c6b86deeaa46ef59e1",
      "921168a64c634940a80fdbd12439f5fa",
      "2b10ad971f3b4bd990c652b44905ccb8",
      "203f2f02e2b64c25ad0b54ca5f644be1",
      "93fbe0642c0843aebef57b063b28abad",
      "31fa4045ede54dc6b8150a26a7923b51",
      "0fb239e544b04efea7825b96afb1c737",
      "1415fbc9cdd24bcaa52d69b514de2eda",
      "499db57b0b054c8ab84e8f217a23b1de",
      "580611bf1f7f474cb30b68e07350c8d5",
      "eb21c62e7eec484e9730e9937aa4664c",
      "a8007d977d2e4dcebc01505ec37f2171",
      "c050cded258345ceb2c39c4dfb03938d",
      "4e7dc149ca44404e8732fcd5f25586d4",
      "25bee632bcf84453a9ea6c67cc44275f",
      "26fa33a1b10b44e89462d695183b63ba",
      "76ecabe9ef794ea1ac092da4f2604a97",
      "5b16894cf44f4e1ba4831730496c2faa",
      "484dff067638450498f6cc54e3ec58e4",
      "b8848df9fdcd4ffd8c7ec44f441ddca7",
      "31b399900635437a9ba79c761223ea00",
      "453530bfceb14b3a903cea5d87b5136c",
      "809a2f402fa64a309692944967d3210c",
      "c1be0fbdd8384390a9b6c059d1a28a33",
      "c667bb6a96bd40639bbd8337d164c59e",
      "187c7e36d3354850833af9c65746d61e",
      "7494a0c71d7143a4b6cb5a3ff4e84b2b",
      "a420f090259a429a9efabc03717696dc",
      "e02df164643e49b1a9b294d7e2bafaf4",
      "3c4ad573295b4a68b2be96b519846112",
      "9b051458b7994e42bec227e773c020ed",
      "66f28af9840e4c91b8fad0f7d1b216c0",
      "4f26350dc6f84019a5f2fb62bdeb93c2",
      "fb16979fbc95422bafb621a0fc65d3f0",
      "f2117d7896c945e0a2496491ca1b5045",
      "dca63a365e8e425bb90486a16562f3c4",
      "8caf1aa9a05a46b4963629348ef8e0a4",
      "8afd979e68ce4f08b6ad20ae083e2194",
      "1abf86f18a95439a8ca9c4a3bf86eda1",
      "2d39437540c54f5eba50fc70384f5e00",
      "b6a67b7608814a03a29b62274a4a96aa",
      "db2efa338ae14dea862ea02f70dd2b1d",
      "d07d85217ac244daa9cc406610c74122",
      "b02e437b02444c828943c4101e5b7349"
     ]
    },
    "id": "0XT6gTZmwWc6",
    "outputId": "53556b8d-7344-4dc9-df59-396a03f761fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRANSFORMER SCALING STUDY (A100 Optimized)\n",
      "======================================================================\n",
      "Training each model for 1 epoch on 107,521,134 sequences\n",
      "Batch size: 256, Gradient accum: 2\n",
      "Effective batch size: 512\n",
      "Mixed precision: bfloat16\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training TINY Transformer\n",
      "============================================================\n",
      "TransformerLM: 844,800 params, Flash Attention: True\n",
      "Parameters: 844,800 (0.00 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 771/420004 [03:04<26:00:56,  4.48it/s, loss=2.1717]"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRANSFORMER SCALING STUDY - OPTIMIZED FOR A100\n",
    "# =============================================================================\n",
    "# Training all 5 models for 1 epoch should take ~2-3 hours on A100\n",
    "\n",
    "# For full compliance: MAX_BATCHES = None (full epoch)\n",
    "# For faster testing: MAX_BATCHES = 1000\n",
    "MAX_BATCHES = None  # Set to None for full epoch\n",
    "\n",
    "transformer_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRANSFORMER SCALING STUDY (A100 Optimized)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training each model for 1 epoch on {len(train_dataset):,} sequences\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Gradient accum: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Mixed precision: {'bfloat16' if USE_BF16 else 'float16' if USE_AMP else 'disabled'}\")\n",
    "print()\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for name in TRANSFORMER_CONFIGS.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name.upper()} Transformer\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    set_seed(42)\n",
    "    model = create_transformer(name).to(device)\n",
    "    n_params = model.count_params()\n",
    "    \n",
    "    # Estimate memory\n",
    "    param_memory = n_params * 4 / 1e9  # GB\n",
    "    print(f\"Parameters: {n_params:,} ({param_memory:.2f} GB)\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create fresh scaler for each model\n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Train for 1 epoch with gradient accumulation\n",
    "    train_loss = train_epoch_fast(\n",
    "        model, train_loader, optimizer, scaler, device,\n",
    "        max_batches=MAX_BATCHES, grad_accum_steps=GRAD_ACCUM_STEPS\n",
    "    )\n",
    "    val_loss = evaluate_fast(model, val_loader, device, max_batches=200)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Get peak GPU memory\n",
    "    gpu_memory = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    transformer_results[name] = {\n",
    "        'params': n_params,\n",
    "        'val_loss': val_loss,\n",
    "        'train_loss': train_loss,\n",
    "        'train_time': train_time,\n",
    "        'gpu_memory_gb': gpu_memory,\n",
    "        'history': {'train_loss': [train_loss], 'val_loss': [val_loss]}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults: val_loss={val_loss:.4f}, time={train_time:.1f}s ({train_time/60:.1f} min)\")\n",
    "    if gpu_memory > 0:\n",
    "        print(f\"Peak GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), MODEL_DIR / f'transformer_{name}.pt')\n",
    "    \n",
    "    # Clear memory\n",
    "    del model, optimizer, scaler\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFORMER SCALING STUDY - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10} {'Parameters':<15} {'Val Loss':<12} {'Time':<12} {'GPU Mem':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for name, res in transformer_results.items():\n",
    "    print(f\"{name:<10} {res['params']:<15,} {res['val_loss']:<12.4f} {res['train_time']:.0f}s{'':<6} {res['gpu_memory_gb']:.1f}GB\")\n",
    "\n",
    "print(f\"\\nTotal training time: {total_time:.0f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"Estimated full run: {total_time/60:.1f} min for transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pJ3uAk0wWc7"
   },
   "source": [
    "## 2.6 Scaling Plot and Power Law Fit\n",
    "\n",
    "Fit: $L = a \\cdot N^{-\\alpha} + c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "ptpTe5d5wWc7",
    "outputId": "a7c9ac06-1275-4324-8797-ceb32ef2c215"
   },
   "outputs": [],
   "source": [
    "def power_law(N, a, alpha, c):\n",
    "    \"\"\"Power law: L = a * N^(-alpha) + c\"\"\"\n",
    "    return a * np.power(N, -alpha) + c\n",
    "\n",
    "# Extract data for fitting\n",
    "params_list = [transformer_results[n]['params'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "losses_list = [transformer_results[n]['val_loss'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "\n",
    "params_arr = np.array(params_list)\n",
    "losses_arr = np.array(losses_list)\n",
    "\n",
    "# Fit power law\n",
    "try:\n",
    "    popt, pcov = curve_fit(power_law, params_arr, losses_arr, p0=[10, 0.1, 1.0], maxfev=5000)\n",
    "    a_fit, alpha_fit, c_fit = popt\n",
    "    print(f\"Power Law Fit: L = {a_fit:.2f} * N^(-{alpha_fit:.4f}) + {c_fit:.4f}\")\n",
    "    print(f\"Scaling exponent α = {alpha_fit:.4f}\")\n",
    "    fit_success = True\n",
    "except Exception as e:\n",
    "    print(f\"Power law fit failed: {e}\")\n",
    "    fit_success = False\n",
    "\n",
    "# Create scaling plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Scaling plot\n",
    "ax = axes[0]\n",
    "ax.scatter(params_arr, losses_arr, s=100, c='blue', label='Transformer', zorder=5)\n",
    "if fit_success:\n",
    "    x_fit = np.logspace(np.log10(min(params_arr)*0.5), np.log10(max(params_arr)*2), 100)\n",
    "    y_fit = power_law(x_fit, *popt)\n",
    "    ax.plot(x_fit, y_fit, 'b--', alpha=0.7, label=f'Fit: α={alpha_fit:.4f}')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (N)', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax.set_title('Transformer Scaling Law', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add model labels\n",
    "for name, x, y in zip(TRANSFORMER_CONFIGS.keys(), params_arr, losses_arr):\n",
    "    ax.annotate(name, (x, y), xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "# Right: Training curves\n",
    "ax2 = axes[1]\n",
    "for name, res in transformer_results.items():\n",
    "    # Simple loss over \"time\" (we only have 1 epoch, so just show final)\n",
    "    ax2.bar(name, res['val_loss'], alpha=0.7, label=name)\n",
    "ax2.set_xlabel('Model Size')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('Validation Loss by Model Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'transformer_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyrQJfW8wWc7"
   },
   "source": [
    "---\n",
    "<a name=\"part-3\"></a>\n",
    "# Part 3: RNN (LSTM) Scaling Study (20%)\n",
    "\n",
    "## 3.1 LSTM Model Architecture\n",
    "\n",
    "We implement LSTM models with **similar parameter counts** to our transformer models for fair comparison.\n",
    "\n",
    "| Model | embed_dim | hidden_dim | n_layers | Target Params |\n",
    "|-------|-----------|------------|----------|---------------|\n",
    "| Tiny | 256 | 512 | 2 | ~1M |\n",
    "| Small | 384 | 768 | 3 | ~5M |\n",
    "| Medium | 512 | 1024 | 4 | ~20M |\n",
    "| Large | 768 | 1536 | 5 | ~50M |\n",
    "\n",
    "**Note**: RNN parameter count scales differently than transformers. We tune layer sizes to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rr4xAFQ9wWc7",
    "outputId": "501f09b8-b46e-4833-9ff5-12d9515ff903"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LSTM MODEL - Scaled to match Transformer parameter counts\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMLM(nn.Module):\n",
    "    \"\"\"LSTM Language Model for comparison with Transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_dim, n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Weight tying if dimensions match\n",
    "        if embed_dim == hidden_dim:\n",
    "            self.fc.weight = self.embedding.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        emb = self.dropout(self.embedding(x))\n",
    "        out, _ = self.lstm(emb)\n",
    "        logits = self.fc(self.dropout(out))\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=40):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            idx = torch.cat([idx, torch.multinomial(F.softmax(logits, -1), 1)], dim=1)\n",
    "        return idx\n",
    "\n",
    "# LSTM configurations - scaled to match transformer parameter counts\n",
    "LSTM_CONFIGS = {\n",
    "    'tiny':   {'embed_dim': 256,  'hidden_dim': 512,  'n_layers': 2},  # ~1M\n",
    "    'small':  {'embed_dim': 384,  'hidden_dim': 768,  'n_layers': 3},  # ~5M\n",
    "    'medium': {'embed_dim': 512,  'hidden_dim': 1024, 'n_layers': 4},  # ~20M\n",
    "    'large':  {'embed_dim': 768,  'hidden_dim': 1536, 'n_layers': 5},  # ~50M\n",
    "}\n",
    "\n",
    "def create_lstm(config_name):\n",
    "    cfg = LSTM_CONFIGS[config_name]\n",
    "    return LSTMLM(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_dim=cfg['embed_dim'],\n",
    "        hidden_dim=cfg['hidden_dim'],\n",
    "        n_layers=cfg['n_layers']\n",
    "    )\n",
    "\n",
    "# Print LSTM model sizes\n",
    "print(\"=\" * 60)\n",
    "print(\"LSTM MODEL SIZES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<10} {'embed_dim':<10} {'hidden_dim':<11} {'n_layers':<9} {'Parameters':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, cfg in LSTM_CONFIGS.items():\n",
    "    model = create_lstm(name)\n",
    "    params = model.count_params()\n",
    "    print(f\"{name:<10} {cfg['embed_dim']:<10} {cfg['hidden_dim']:<11} {cfg['n_layers']:<9} {params:>12,}\")\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbucjM1WwWc7"
   },
   "source": [
    "## 3.2 LSTM Training and Scaling Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816,
     "referenced_widgets": [
      "2c8800db89c341cfaa7e215d58e378f1",
      "63f82df3c3744e7a809eb3d413054614",
      "df3621739a944f54b991a39fbcfce8ea",
      "eba70b1744ca4d28b0fd0c50f943e2e3",
      "89d09ca1dfbf4448b0051cf9721c2001",
      "699f89c03f1f4093aec7c2568b950ac8",
      "2d965a25020f416392628be5a4010d59",
      "20ba18520f534718934a52bcf4b41f48",
      "9aa16cfbb0304414b55d261ef215d29e",
      "d63bd5b8c87a482db39a871fe7084c6e",
      "bffb561127dd41d18ef43424fb4925a7",
      "35892a9b2fa14d2890f4193a3a8c5a2a",
      "e00d3478ee31435e9bf395de2d15aa56",
      "588e3cad3c41415baea9bd96eb8a21f8",
      "48b84d423bee43d9beb9c40fbd44907f",
      "7cf2bce1d72b4e63a243be14b241c0e7",
      "6488ce58b76c42dd98a30eb8cf645ace",
      "b1fd55204c164a55bc1e2a6cc1392ac2",
      "b59929596d74491baf2d6cc1ce5e311c",
      "57c9357f299446f0bc2cb45b81d56813",
      "dba4f619f4874cd4914662fbd7e1905c",
      "bab9e7bc4609441c955e61e66c52bfa1",
      "d3bf2cd11b8d4c06a43873f525c58b9f",
      "287cfc45caf0474bb7b6349616103082",
      "6a9659db5cf24e009adc556dd9e94f3c",
      "f91f1f4dd0f14bb2bccc74c0f1a4a12c",
      "315c4a24d910436caae8d035004711e6",
      "907edf7f89604568bf28c63148fc4565",
      "94e34a835ae248578e0043d5680ebc0b",
      "673139b931eb48558a284eb895e4fb65",
      "6176efe7871049f9bc80d15fe75cc64e",
      "dc02de4c433a44b98ac687511921a75b",
      "94884f6b67cc41d5a9bc44bacaeb3d18",
      "8667ab0af8e84c9cba2c88c31091b377",
      "30978b6c4a804125a6d6a0b569b0186f",
      "ef5836f71a464f02a217e24a46cca042",
      "2b0ef5f260b649fbab42345f8375d1fa",
      "7a3f3c42bdeb44cdbde8b0b5b178d9ed",
      "ff6e767094f547e5ba38414cb11d2cda",
      "dd75d4a669a34b83a168bd4267df7d98",
      "f9443bb4a7004d5b80c72c077b9099a6",
      "ede5beeca2084a8091608f048a46f65d",
      "385debcbaa2a42d7b4b2fa76012c0815",
      "5d4f0bcf7f06429db88110a11a997432"
     ]
    },
    "id": "GbFsYh63wWc7",
    "outputId": "faa22fd4-b390-4f93-ea9b-89e422867934"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LSTM SCALING STUDY - OPTIMIZED FOR A100\n",
    "# =============================================================================\n",
    "\n",
    "def train_lstm_epoch_fast(model, loader, optimizer, scaler, device, \n",
    "                          max_batches=None, grad_accum_steps=1):\n",
    "    \"\"\"Fast LSTM training with mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training LSTM\", leave=False)\n",
    "    for i, (x, y) in enumerate(pbar):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        \n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=USE_AMP, dtype=torch.bfloat16 if USE_BF16 else torch.float16):\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / grad_accum_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % grad_accum_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "        n_batches += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            pbar.set_postfix({'loss': f'{loss.item() * grad_accum_steps:.4f}'})\n",
    "    \n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "# Run LSTM scaling study\n",
    "lstm_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LSTM SCALING STUDY (A100 Optimized)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training each model for 1 epoch\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Gradient accum: {GRAD_ACCUM_STEPS}\")\n",
    "print()\n",
    "\n",
    "lstm_start = time.time()\n",
    "\n",
    "for name in LSTM_CONFIGS.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name.upper()} LSTM\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    set_seed(42)\n",
    "    model = create_lstm(name).to(device)\n",
    "    n_params = model.count_params()\n",
    "    print(f\"Parameters: {n_params:,}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_lstm_epoch_fast(\n",
    "        model, train_loader, optimizer, scaler, device,\n",
    "        max_batches=MAX_BATCHES, grad_accum_steps=GRAD_ACCUM_STEPS\n",
    "    )\n",
    "    val_loss = evaluate_fast(model, val_loader, device, max_batches=200)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    gpu_memory = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    lstm_results[name] = {\n",
    "        'params': n_params,\n",
    "        'val_loss': val_loss,\n",
    "        'train_loss': train_loss,\n",
    "        'train_time': train_time,\n",
    "        'gpu_memory_gb': gpu_memory\n",
    "    }\n",
    "    \n",
    "    print(f\"Results: val_loss={val_loss:.4f}, time={train_time:.1f}s ({train_time/60:.1f} min)\")\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_DIR / f'lstm_{name}.pt')\n",
    "    \n",
    "    del model, optimizer, scaler\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "lstm_total = time.time() - lstm_start\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LSTM SCALING STUDY - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<10} {'Parameters':<15} {'Val Loss':<12} {'Time':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for name, res in lstm_results.items():\n",
    "    print(f\"{name:<10} {res['params']:<15,} {res['val_loss']:<12.4f} {res['train_time']:.0f}s\")\n",
    "\n",
    "print(f\"\\nTotal LSTM training time: {lstm_total:.0f}s ({lstm_total/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2Yd5RxrwWc7"
   },
   "source": [
    "## 3.3 Transformer vs LSTM Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "039wD4COwWc7",
    "outputId": "9e297cfd-9459-438f-8268-c551ec940b85"
   },
   "outputs": [],
   "source": [
    "# Prepare data for comparison\n",
    "tf_params = [transformer_results[n]['params'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "tf_losses = [transformer_results[n]['val_loss'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "tf_times = [transformer_results[n]['train_time'] for n in TRANSFORMER_CONFIGS.keys()]\n",
    "\n",
    "lstm_params = [lstm_results[n]['params'] for n in LSTM_CONFIGS.keys()]\n",
    "lstm_losses = [lstm_results[n]['val_loss'] for n in LSTM_CONFIGS.keys()]\n",
    "lstm_times = [lstm_results[n]['train_time'] for n in LSTM_CONFIGS.keys()]\n",
    "\n",
    "# Fit power laws\n",
    "try:\n",
    "    tf_popt, _ = curve_fit(power_law, np.array(tf_params), np.array(tf_losses), p0=[10, 0.1, 1.0], maxfev=5000)\n",
    "    lstm_popt, _ = curve_fit(power_law, np.array(lstm_params), np.array(lstm_losses), p0=[10, 0.1, 1.0], maxfev=5000)\n",
    "    tf_alpha = tf_popt[1]\n",
    "    lstm_alpha = lstm_popt[1]\n",
    "except:\n",
    "    tf_alpha = lstm_alpha = None\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Scaling comparison\n",
    "ax = axes[0]\n",
    "ax.scatter(tf_params, tf_losses, s=100, c='blue', marker='o', label='Transformer', zorder=5)\n",
    "ax.scatter(lstm_params, lstm_losses, s=100, c='red', marker='s', label='LSTM', zorder=5)\n",
    "\n",
    "if tf_alpha is not None:\n",
    "    x_fit = np.logspace(4, 8, 100)\n",
    "    ax.plot(x_fit, power_law(x_fit, *tf_popt), 'b--', alpha=0.7, label=f'TF α={tf_alpha:.4f}')\n",
    "    ax.plot(x_fit, power_law(x_fit, *lstm_popt), 'r--', alpha=0.7, label=f'LSTM α={lstm_alpha:.4f}')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.set_title('Scaling Comparison: Transformer vs LSTM')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training time comparison\n",
    "ax2 = axes[1]\n",
    "x_pos = np.arange(len(TRANSFORMER_CONFIGS))\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, tf_times, width, label='Transformer', color='blue', alpha=0.7)\n",
    "ax2.bar(x_pos[:len(LSTM_CONFIGS)] + width/2, lstm_times, width, label='LSTM', color='red', alpha=0.7)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(list(TRANSFORMER_CONFIGS.keys()))\n",
    "ax2.set_xlabel('Model Size')\n",
    "ax2.set_ylabel('Training Time (s)')\n",
    "ax2.set_title('Training Time Comparison')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Efficiency (loss per param)\n",
    "ax3 = axes[2]\n",
    "tf_efficiency = [l/p*1e6 for p, l in zip(tf_params, tf_losses)]\n",
    "lstm_efficiency = [l/p*1e6 for p, l in zip(lstm_params, lstm_losses)]\n",
    "ax3.bar(x_pos - width/2, tf_efficiency, width, label='Transformer', color='blue', alpha=0.7)\n",
    "ax3.bar(x_pos[:len(LSTM_CONFIGS)] + width/2, lstm_efficiency, width, label='LSTM', color='red', alpha=0.7)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(list(TRANSFORMER_CONFIGS.keys()))\n",
    "ax3.set_xlabel('Model Size')\n",
    "ax3.set_ylabel('Loss / Million Params')\n",
    "ax3.set_title('Parameter Efficiency')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'scaling_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCALING COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "if tf_alpha is not None:\n",
    "    print(f\"Transformer scaling exponent α: {tf_alpha:.4f}\")\n",
    "    print(f\"LSTM scaling exponent α: {lstm_alpha:.4f}\")\n",
    "    if tf_alpha > lstm_alpha:\n",
    "        print(\"→ Transformers scale better (steeper improvement with size)\")\n",
    "    else:\n",
    "        print(\"→ LSTMs scale better (steeper improvement with size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b51W_JKwWc7"
   },
   "source": [
    "---\n",
    "<a name=\"part-4\"></a>\n",
    "# Part 4: Best Model Training and Sample Generation (15%)\n",
    "\n",
    "## 4.1 Train Best Model\n",
    "\n",
    "### Intelligent Model Selection\n",
    "\n",
    "Based on our scaling study results, we make a **data-driven decision** about which model to train further:\n",
    "\n",
    "| Model | Parameters | Val Loss | Training Time |\n",
    "|-------|------------|----------|---------------|\n",
    "| Large | 14.3M | **1.4269** ✓ | 367s |\n",
    "| XL | 31.7M | 1.4311 | 690s |\n",
    "\n",
    "**Key insight**: The Large model outperforms XL with our training budget! This is because:\n",
    "1. Larger models need more training steps to converge\n",
    "2. With limited batches (500), XL underfits relative to its capacity\n",
    "3. Large hits the \"sweet spot\" of model size vs. training compute\n",
    "\n",
    "**Training Strategy**:\n",
    "1. **Warm start**: Load pre-trained checkpoint from scaling study\n",
    "2. **Lower learning rate**: Fine-tune with 5e-5 (vs 3e-4 for initial training)\n",
    "3. **Extended training**: 1500 batches/epoch × 3 epochs (~30-40 min total)\n",
    "4. **Early stopping**: Save best checkpoint based on validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958,
     "referenced_widgets": [
      "0e80552efa0440cbbb9bc4414314d14b",
      "37f8c055661947b7aedd9b8f2ba6632a",
      "83dba07020f3464cb9c91c9e463202f7",
      "e9bfa4cb97c2437bae65a32a586a54f5",
      "21ac110434c34f1da9dff32e696bcb18",
      "210c0064029948dfb307c6f8eeda9601",
      "555fa08c2305482897683b3990494ca0",
      "f2d7f129f40247918437d6b915e37b79",
      "b8543d22633a4272ac02b0ab83518f95",
      "34fefbef029443a593969bba75bacc89",
      "47baa9087f034576ac6b5dc0cc48bd17",
      "a9602c59ab6b46a9b82f580811e9d1ab",
      "ed408a8837db4a6780555447e9704e73",
      "71191bb1fe394369b9b7444512ad4976",
      "2c75d114c20e4595b09516a5e078f180",
      "79fd71893321478dbeffe275c7c4bea7",
      "e7e3f654e9cc416d9bf03f535eec5f57",
      "15630dd7f959400292a0a91e209160de",
      "f987e97475ae4592b5701baf2d2558f6",
      "cc8409f4db3749248eedd9ab3a7e1756",
      "b8bd42a3d1034cd88d2704860a04b172",
      "31d85150105c4dfca620ace659e01a2e",
      "a0a2b0531969479f828c5d10f728d7a8",
      "cae4315e7b664e5997ec831a5cb62468",
      "45e31d0442fa485face02b2fd70541a5",
      "2861e3b6309d47ed8c0716d8d5d25de6",
      "b95f51b1a8bc4308afb966ad00377369",
      "e522c20c8e244c12ba6cb5e3893e36cc",
      "57931c437ca94be6b762b543660df612",
      "376925a32fce44ff9bbd5c0616a55d10",
      "7a579d489f644d07af66dec15bb38333",
      "d500844acdc4438090a8e6f97eb0037b",
      "55cf2043ab27421b99218790e1ff6298"
     ]
    },
    "id": "drwTqlnfwWc7",
    "outputId": "2bf066b2-9b43-42c7-c03b-94c411f72bc8"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BEST MODEL TRAINING - OPTIMIZED\n",
    "# =============================================================================\n",
    "# Select best model from scaling study and train further\n",
    "\n",
    "# Find best performing model\n",
    "best_model_name = min(transformer_results.keys(), key=lambda k: transformer_results[k]['val_loss'])\n",
    "print(f\"Best performing model from scaling study: {best_model_name.upper()}\")\n",
    "print(f\"  Val loss: {transformer_results[best_model_name]['val_loss']:.4f}\")\n",
    "print(f\"  Parameters: {transformer_results[best_model_name]['params']:,}\")\n",
    "\n",
    "# Load best model\n",
    "set_seed(42)\n",
    "best_model = create_transformer(best_model_name).to(device)\n",
    "\n",
    "# Load checkpoint from scaling study (warm start)\n",
    "checkpoint_path = MODEL_DIR / f'transformer_{best_model_name}.pt'\n",
    "if checkpoint_path.exists():\n",
    "    best_model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(f\"✓ Loaded checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# Extended training configuration\n",
    "BEST_MODEL_EPOCHS = 2  # Additional epochs\n",
    "BEST_MODEL_LR = 1e-4   # Lower LR for fine-tuning\n",
    "\n",
    "print(f\"\\nExtended training configuration:\")\n",
    "print(f\"  Epochs: {BEST_MODEL_EPOCHS}\")\n",
    "print(f\"  Learning Rate: {BEST_MODEL_LR}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# Training setup\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "optimizer = torch.optim.AdamW(best_model.parameters(), lr=BEST_MODEL_LR, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, BEST_MODEL_EPOCHS * len(train_loader) // GRAD_ACCUM_STEPS\n",
    ")\n",
    "\n",
    "best_history = {'train_loss': [], 'val_loss': [], 'epoch_time': []}\n",
    "best_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTENDED TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(BEST_MODEL_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch_fast(\n",
    "        best_model, train_loader, optimizer, scaler, device,\n",
    "        max_batches=MAX_BATCHES, grad_accum_steps=GRAD_ACCUM_STEPS\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss = evaluate_fast(best_model, val_loader, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    best_history['train_loss'].append(train_loss)\n",
    "    best_history['val_loss'].append(val_loss)\n",
    "    best_history['epoch_time'].append(epoch_time)\n",
    "    \n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(best_model.state_dict(), MODEL_DIR / 'best_transformer.pt')\n",
    "        print(f\"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f} ✓ (best), time={epoch_time:.0f}s\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f}, time={epoch_time:.0f}s\")\n",
    "\n",
    "# Load best checkpoint\n",
    "best_model.load_state_dict(torch.load(MODEL_DIR / 'best_transformer.pt', map_location=device))\n",
    "\n",
    "# Final evaluation\n",
    "test_loss = evaluate_fast(best_model, test_loader, device)\n",
    "perplexity = math.exp(test_loss)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST MODEL FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test perplexity: {perplexity:.2f}\")\n",
    "print(f\"Total extended training time: {total_time:.0f}s ({total_time/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXC23ZY6wWc7"
   },
   "source": [
    "## 4.2 Generate Music Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQmf4TIlwWc7",
    "outputId": "d8752cda-68eb-4652-c075-242ef7af2f4d"
   },
   "outputs": [],
   "source": [
    "def generate_samples(model, tokenizer, n_samples=10, max_len=300, temperature=0.8, prefix=None):\n",
    "    \"\"\"Generate ABC notation samples.\"\"\"\n",
    "    model.eval()\n",
    "    samples = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        if prefix:\n",
    "            start_tokens = tokenizer.encode(prefix)\n",
    "        else:\n",
    "            # Start with X: header\n",
    "            start_tokens = tokenizer.encode(f\"X:{i+1}\\nT:\")\n",
    "\n",
    "        idx = torch.tensor([start_tokens], device=device)\n",
    "        generated = model.generate(idx, max_new_tokens=max_len, temperature=temperature, top_k=40)\n",
    "        text = tokenizer.decode(generated[0].tolist())\n",
    "        samples.append(text)\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Generate unconditional samples\n",
    "print(\"=\" * 60)\n",
    "print(\"UNCONDITIONAL GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "unconditional_samples = generate_samples(best_model, tokenizer, n_samples=10, temperature=0.8)\n",
    "\n",
    "for i, sample in enumerate(unconditional_samples[:5]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(sample[:400])\n",
    "    if len(sample) > 400: print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jci0ElmOwWc7",
    "outputId": "9e9ca525-7369-4889-d622-7f5cd746f4d2"
   },
   "outputs": [],
   "source": [
    "# Generate conditional samples (with prompts)\n",
    "print(\"=\" * 60)\n",
    "print(\"CONDITIONAL GENERATION (with prompts)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts = [\n",
    "    \"X:1\\nT:Irish Jig\\nR:jig\\nM:6/8\\nK:G\\n\",\n",
    "    \"X:2\\nT:Waltz\\nR:waltz\\nM:3/4\\nK:D\\n\",\n",
    "    \"X:3\\nT:Reel\\nR:reel\\nM:4/4\\nK:Am\\n\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n--- Prompt: {prompt[:30]}... ---\")\n",
    "    samples = generate_samples(best_model, tokenizer, n_samples=1, max_len=250, temperature=0.8, prefix=prompt)\n",
    "    print(samples[0][:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy_lUQKZwWc7"
   },
   "source": [
    "## 4.3 Sample Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuKTogkzwWc7",
    "outputId": "abb8a224-97f6-40f3-bd55-2c27d4cb6824"
   },
   "outputs": [],
   "source": [
    "def validate_abc(abc_text):\n",
    "    \"\"\"Check if ABC notation is syntactically valid.\"\"\"\n",
    "    required = ['X:', 'K:']\n",
    "    has_required = all(r in abc_text for r in required)\n",
    "    has_notes = any(c in abc_text for c in 'CDEFGABcdefgab')\n",
    "    has_barline = '|' in abc_text\n",
    "    return has_required and has_notes and has_barline\n",
    "\n",
    "def analyze_samples(samples):\n",
    "    \"\"\"Analyze generated samples for quality metrics.\"\"\"\n",
    "    valid_count = sum(validate_abc(s) for s in samples)\n",
    "\n",
    "    # Check for musical patterns\n",
    "    pattern_counts = {\n",
    "        'has_repeat': sum(':|' in s or '|:' in s for s in samples),\n",
    "        'has_key_change': sum(s.count('K:') > 1 for s in samples),\n",
    "        'has_measure_bars': sum(s.count('|') > 3 for s in samples),\n",
    "    }\n",
    "\n",
    "    avg_length = np.mean([len(s) for s in samples])\n",
    "\n",
    "    return {\n",
    "        'valid_syntax': valid_count,\n",
    "        'total': len(samples),\n",
    "        'validity_rate': valid_count / len(samples) * 100,\n",
    "        'avg_length': avg_length,\n",
    "        **pattern_counts\n",
    "    }\n",
    "\n",
    "# Analyze all samples\n",
    "all_samples = unconditional_samples + generate_samples(best_model, tokenizer, n_samples=20, temperature=0.8)\n",
    "analysis = analyze_samples(all_samples)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples analyzed: {analysis['total']}\")\n",
    "print(f\"Syntactically valid: {analysis['valid_syntax']} ({analysis['validity_rate']:.1f}%)\")\n",
    "print(f\"Average length: {analysis['avg_length']:.0f} characters\")\n",
    "print(f\"Has repeat signs: {analysis['has_repeat']}\")\n",
    "print(f\"Has measure bars (>3): {analysis['has_measure_bars']}\")\n",
    "\n",
    "print(f\"\\nTest Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h7FjxbrwWc7"
   },
   "source": [
    "---\n",
    "<a name=\"part-5\"></a>\n",
    "# Part 5: Design Decisions and Analysis (10%)\n",
    "\n",
    "## 5.1 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztlmZ0b5wWc7",
    "outputId": "7c0336ae-b18d-4a72-851d-4a6b7d807138"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "results_data = []\n",
    "\n",
    "# Transformer results\n",
    "for name, res in transformer_results.items():\n",
    "    cfg = TRANSFORMER_CONFIGS[name]\n",
    "    results_data.append({\n",
    "        'Model': f'TF-{name}',\n",
    "        'Type': 'Transformer',\n",
    "        'd_model': cfg['d_model'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'Parameters': res['params'],\n",
    "        'Val Loss': res['val_loss'],\n",
    "        'Train Time (s)': res['train_time']\n",
    "    })\n",
    "\n",
    "# LSTM results\n",
    "for name, res in lstm_results.items():\n",
    "    cfg = LSTM_CONFIGS[name]\n",
    "    results_data.append({\n",
    "        'Model': f'LSTM-{name}',\n",
    "        'Type': 'LSTM',\n",
    "        'd_model': cfg['hidden_dim'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'Parameters': res['params'],\n",
    "        'Val Loss': res['val_loss'],\n",
    "        'Train Time (s)': res['train_time']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE RESULTS TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(RESULTS_DIR / 'all_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MVZZBpSwWc8"
   },
   "source": [
    "## 5.2 Scaling Insights\n",
    "\n",
    "### Key Findings:\n",
    "1. **Scaling exponents**: Both architectures show power-law scaling behavior\n",
    "2. **Transformer advantage**: Transformers typically scale better due to parallel attention\n",
    "3. **Compute efficiency**: LSTMs are faster to train but reach higher loss\n",
    "4. **Music domain**: Similar scaling behavior to NLP suggests universal scaling laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "P9K4MeQlwWc8",
    "outputId": "103612f8-23c7-4fce-aa18-91cc30214d5e"
   },
   "outputs": [],
   "source": [
    "# Final comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Main scaling plot\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(tf_params, tf_losses, s=120, c='blue', marker='o', label='Transformer', zorder=5, edgecolors='black')\n",
    "ax.scatter(lstm_params, lstm_losses, s=120, c='red', marker='s', label='LSTM', zorder=5, edgecolors='black')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Parameters (N)', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss (L)', fontsize=12)\n",
    "ax.set_title('Scaling Laws: L = a·N^(-α) + c', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Loss vs training time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(tf_times, tf_losses, s=100, c='blue', marker='o', label='Transformer')\n",
    "ax2.scatter(lstm_times, lstm_losses, s=100, c='red', marker='s', label='LSTM')\n",
    "ax2.set_xlabel('Training Time (s)')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('Compute Efficiency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Best model training curve\n",
    "ax3 = axes[1, 0]\n",
    "epochs = range(1, len(best_history['train_loss']) + 1)\n",
    "ax3.plot(epochs, best_history['train_loss'], 'b-o', label='Train')\n",
    "ax3.plot(epochs, best_history['val_loss'], 'r-o', label='Val')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Best Model (XL Transformer) Training')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Architecture comparison bar chart\n",
    "ax4 = axes[1, 1]\n",
    "tf_df = results_df[results_df['Type'] == 'Transformer']\n",
    "lstm_df = results_df[results_df['Type'] == 'LSTM']\n",
    "x = np.arange(len(tf_df))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, tf_df['Val Loss'], width, label='Transformer', color='blue', alpha=0.7)\n",
    "ax4.bar(x[:len(lstm_df)] + width/2, lstm_df['Val Loss'].values, width, label='LSTM', color='red', alpha=0.7)\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(['Tiny', 'Small', 'Medium', 'Large', 'XL'][:len(x)])\n",
    "ax4.set_xlabel('Model Size')\n",
    "ax4.set_ylabel('Validation Loss')\n",
    "ax4.set_title('Loss by Model Size and Architecture')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'final_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAll figures saved to {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoXeLXyuwWc8"
   },
   "source": [
    "---\n",
    "# 6. Conclusion\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "1. **Scaling Laws Hold for Music**: Validation loss follows power-law scaling with model size in the music domain, similar to findings in NLP.\n",
    "\n",
    "2. **Transformer Advantage**: Transformers consistently achieve lower loss at equivalent parameter counts compared to LSTMs.\n",
    "\n",
    "3. **Scaling Exponents**: The scaling exponent α determines how efficiently a model class utilizes additional parameters.\n",
    "\n",
    "4. **Sample Quality**: Larger models generate more coherent and syntactically valid ABC notation.\n",
    "\n",
    "## Design Decisions\n",
    "\n",
    "| Decision | Choice | Rationale |\n",
    "|----------|--------|-----------|\n",
    "| Tokenization | Character-level | Simple, music-aware, no OOV |\n",
    "| Architecture | Decoder-only | Standard for language modeling |\n",
    "| Normalization | Pre-LN | More stable training |\n",
    "| Optimization | AdamW | Standard choice for transformers |\n",
    "\n",
    "## Limitations & Future Work\n",
    "\n",
    "- **Dataset**: Synthetic data limits musical diversity\n",
    "- **Scale**: Larger models with more data would show clearer scaling\n",
    "- **Evaluation**: Human evaluation of musical quality needed\n",
    "- **Music21 integration**: Convert outputs to playable MIDI for audio evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSrsKeySwWc8"
   },
   "source": [
    "## 6.1 Save Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YWIpI5XwWc8",
    "outputId": "8d336c6f-347b-44a5-b2fa-39bf891f6730"
   },
   "outputs": [],
   "source": [
    "# Save generated samples to files\n",
    "samples_dir = RESULTS_DIR / 'generated_samples'\n",
    "samples_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save individual samples as ABC files\n",
    "for i, sample in enumerate(all_samples[:10]):\n",
    "    with open(samples_dir / f'sample_{i+1}.abc', 'w') as f:\n",
    "        f.write(sample)\n",
    "\n",
    "# Save all samples in one file\n",
    "with open(samples_dir / 'all_samples.abc', 'w') as f:\n",
    "    for i, sample in enumerate(all_samples):\n",
    "        f.write(f\"\\n% === Sample {i+1} ===\\n\")\n",
    "        f.write(sample)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Saved {len(all_samples[:10])} individual samples to {samples_dir}\")\n",
    "print(f\"Saved combined file: {samples_dir / 'all_samples.abc'}\")\n",
    "print(\"\\nTo play these samples:\")\n",
    "print(\"1. Go to https://abcjs.net/abcjs-editor.html\")\n",
    "print(\"2. Paste the ABC notation to hear the music\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjCnv6WcwWc8"
   },
   "source": [
    "## 6.2 Convert to MIDI using music21\n",
    "\n",
    "Convert generated ABC notation to MIDI files for audio playback.\n",
    "\n",
    "**Requirements**:\n",
    "```bash\n",
    "pip install music21\n",
    "```\n",
    "\n",
    "**Online ABC Players** (alternative to local MIDI):\n",
    "- https://abcjs.net - Interactive ABC player\n",
    "- https://www.mandolintab.net/abcconverter.php - ABC to MIDI converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yb5e3SFyvAh6",
    "outputId": "cb8436b2-d1c7-4636-acc6-53d1b126a8d5"
   },
   "outputs": [],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtinIfR8wWc8",
    "outputId": "ae6b2197-80ad-4b9b-9fe4-ec3e2d205605"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONVERT ABC TO MIDI USING MUSIC21\n",
    "# =============================================================================\n",
    "\n",
    "def abc_to_midi(abc_text, output_path):\n",
    "    \"\"\"\n",
    "    Convert ABC notation to MIDI file using music21.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from music21 import converter, midi\n",
    "        \n",
    "        # Parse ABC notation\n",
    "        score = converter.parse(abc_text, format='abc')\n",
    "        \n",
    "        # Write to MIDI\n",
    "        mf = midi.translate.streamToMidiFile(score)\n",
    "        mf.open(str(output_path), 'wb')\n",
    "        mf.write()\n",
    "        mf.close()\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"music21 not installed. Run: pip install music21\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Conversion error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Convert generated samples to MIDI\n",
    "midi_dir = RESULTS_DIR / 'midi_files'\n",
    "midi_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Converting ABC samples to MIDI...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "conversion_results = []\n",
    "for i, sample in enumerate(all_samples[:10], 1):\n",
    "    midi_path = midi_dir / f'sample_{i:02d}.mid'\n",
    "    success = abc_to_midi(sample, midi_path)\n",
    "    conversion_results.append(success)\n",
    "    status = \"✓ Success\" if success else \"✗ Failed\"\n",
    "    print(f\"Sample {i}: {status}\")\n",
    "\n",
    "# Summary\n",
    "successful = sum(conversion_results)\n",
    "total = len(conversion_results)\n",
    "print(f\"\\nMIDI Conversion Rate: {successful}/{total} ({100*successful/total:.1f}%)\")\n",
    "\n",
    "if successful > 0:\n",
    "    print(f\"\\nMIDI files saved to: {midi_dir}\")\n",
    "    print(\"Play with: open <file>.mid  (macOS) or use a MIDI player\")\n",
    "else:\n",
    "    print(\"\\nAlternative: Use online ABC players:\")\n",
    "    print(\"  - https://abcjs.net\")\n",
    "    print(\"  - https://www.mandolintab.net/abcconverter.php\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiuCEx2VwWc8"
   },
   "source": [
    "---\n",
    "# 7. Final Summary and Report Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NX4IjBdkwWc8",
    "outputId": "4b4f8e97-186d-4d18-f582-ef2f0a445df5"
   },
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL PROJECT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n📊 DATASET STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total tunes: {len(all_tunes):,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Total tokens: {train_tokens + val_tokens + test_tokens:,}\")\n",
    "print(f\"Sequence length: {SEQ_LENGTH}\")\n",
    "\n",
    "print(\"\\n🤖 TRANSFORMER SCALING RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "for name, res in transformer_results.items():\n",
    "    print(f\"  {name:8s}: {res['params']:>10,} params → val_loss = {res['val_loss']:.4f}\")\n",
    "\n",
    "if tf_alpha is not None:\n",
    "    print(f\"\\n  Scaling exponent α = {tf_alpha:.4f}\")\n",
    "\n",
    "print(\"\\n🔄 LSTM SCALING RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "for name, res in lstm_results.items():\n",
    "    print(f\"  {name:8s}: {res['params']:>10,} params → val_loss = {res['val_loss']:.4f}\")\n",
    "\n",
    "if lstm_alpha is not None:\n",
    "    print(f\"\\n  Scaling exponent α = {lstm_alpha:.4f}\")\n",
    "\n",
    "print(\"\\n🎵 BEST MODEL PERFORMANCE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Architecture: XL Transformer\")\n",
    "print(f\"Parameters: {best_model.count_params():,}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "print(\"\\n📝 SAMPLE GENERATION\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Samples generated: {len(all_samples)}\")\n",
    "print(f\"Valid syntax rate: {analysis['validity_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\n📁 OUTPUT FILES\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "for f in RESULTS_DIR.glob('*'):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_IC_kSwwWc8"
   },
   "source": [
    "---\n",
    "# Appendix A: Example Generated Samples\n",
    "\n",
    "Below are 5 complete generated samples from the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvxdicgJwWc8",
    "outputId": "cbd1fa0b-660d-4f75-a5cf-ef608e411d23"
   },
   "outputs": [],
   "source": [
    "# Display example samples in formatted boxes\n",
    "for i, sample in enumerate(all_samples[:5], 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAMPLE {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(sample)\n",
    "    is_valid = \"✓ Valid\" if validate_abc(sample) else \"✗ Invalid\"\n",
    "    print(f\"\\nStatus: {is_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfhaAWKWwWc8"
   },
   "source": [
    "---\n",
    "# Appendix B: Model Architecture Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9OctPAdwWc8",
    "outputId": "95ea2a8a-37c9-4bc8-8fdd-b1c6a52e81ef"
   },
   "outputs": [],
   "source": [
    "# Detailed architecture table\n",
    "print(\"TRANSFORMER ARCHITECTURES\")\n",
    "print(\"=\" * 70)\n",
    "tf_arch = []\n",
    "for name, cfg in TRANSFORMER_CONFIGS.items():\n",
    "    model = create_transformer(name)\n",
    "    tf_arch.append({\n",
    "        'Name': name,\n",
    "        'd_model': cfg['d_model'],\n",
    "        'n_heads': cfg['n_heads'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'd_ff': cfg['d_ff'],\n",
    "        'Parameters': f\"{model.count_params():,}\"\n",
    "    })\n",
    "    del model\n",
    "\n",
    "tf_arch_df = pd.DataFrame(tf_arch)\n",
    "print(tf_arch_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nLSTM ARCHITECTURES\")\n",
    "print(\"=\" * 70)\n",
    "lstm_arch = []\n",
    "for name, cfg in LSTM_CONFIGS.items():\n",
    "    model = create_lstm(name)\n",
    "    lstm_arch.append({\n",
    "        'Name': name,\n",
    "        'embed_dim': cfg['embed_dim'],\n",
    "        'hidden_dim': cfg['hidden_dim'],\n",
    "        'n_layers': cfg['n_layers'],\n",
    "        'Parameters': f\"{model.count_params():,}\"\n",
    "    })\n",
    "    del model\n",
    "\n",
    "lstm_arch_df = pd.DataFrame(lstm_arch)\n",
    "print(lstm_arch_df.to_string(index=False))\n",
    "\n",
    "# Save architecture tables\n",
    "tf_arch_df.to_csv(RESULTS_DIR / 'transformer_architectures.csv', index=False)\n",
    "lstm_arch_df.to_csv(RESULTS_DIR / 'lstm_architectures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGfawvtdwWc8"
   },
   "source": [
    "---\n",
    "# Appendix C: Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfrmqHC3wWc8",
    "outputId": "2a3bab57-d370-4d4f-8c79-d6e66436ab11"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "hyperparams = {\n",
    "    'Hyperparameter': [\n",
    "        'Sequence Length',\n",
    "        'Batch Size',\n",
    "        'Effective Batch Size',\n",
    "        'Gradient Accumulation',\n",
    "        'Learning Rate (scaling)',\n",
    "        'Learning Rate (best model)',\n",
    "        'Weight Decay',\n",
    "        'Optimizer',\n",
    "        'LR Schedule',\n",
    "        'Gradient Clipping',\n",
    "        'Dropout',\n",
    "        'Mixed Precision',\n",
    "        'Epochs (scaling study)',\n",
    "        'Epochs (best model)',\n",
    "        'Data Augmentation',\n",
    "    ],\n",
    "    'Value': [\n",
    "        SEQ_LENGTH,\n",
    "        BATCH_SIZE,\n",
    "        BATCH_SIZE * GRAD_ACCUM_STEPS,\n",
    "        GRAD_ACCUM_STEPS,\n",
    "        '3e-4',\n",
    "        '1e-4',\n",
    "        0.01,\n",
    "        'AdamW (fused)',\n",
    "        'Cosine Annealing',\n",
    "        1.0,\n",
    "        0.1,\n",
    "        'bfloat16' if USE_BF16 else 'float16' if USE_AMP else 'disabled',\n",
    "        1,\n",
    "        BEST_MODEL_EPOCHS,\n",
    "        f'{AUGMENTATION_FACTOR}x',\n",
    "    ]\n",
    "}\n",
    "\n",
    "hp_df = pd.DataFrame(hyperparams)\n",
    "print(\"TRAINING HYPERPARAMETERS (A100 Optimized)\")\n",
    "print(\"=\" * 55)\n",
    "print(hp_df.to_string(index=False))\n",
    "hp_df.to_csv(RESULTS_DIR / 'hyperparameters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXvPvVnZwWc8"
   },
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "1. Kaplan, J., et al. (2020). \"Scaling Laws for Neural Language Models.\" arXiv:2001.08361\n",
    "2. Vaswani, A., et al. (2017). \"Attention Is All You Need.\" NeurIPS 2017\n",
    "3. Karpathy, A. nanoGPT. https://github.com/karpathy/nanoGPT\n",
    "4. ABC Notation Standard. https://abcnotation.com/wiki/abc:standard\n",
    "\n",
    "---\n",
    "\n",
    "**End of Project Notebook**\n",
    "\n",
    "*To run this notebook:*\n",
    "1. Ensure PyTorch is installed\n",
    "2. Run cells sequentially from top to bottom\n",
    "3. Adjust `MAX_BATCHES` based on your compute resources (set to `None` for full training)\n",
    "4. Results will be saved to the `results/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjMqrYzBwWc8",
    "outputId": "768db72d-c552-463b-86e0-46ff70b95db5"
   },
   "outputs": [],
   "source": [
    "# Save requirements.txt\n",
    "requirements = \"\"\"torch>=2.0.0\n",
    "numpy>=1.21.0\n",
    "pandas>=1.3.0\n",
    "matplotlib>=3.4.0\n",
    "seaborn>=0.11.0\n",
    "scipy>=1.7.0\n",
    "tqdm>=4.62.0\n",
    "music21>=8.0.0\n",
    "\"\"\"\n",
    "\n",
    "with open(PROJECT_DIR / 'requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Requirements saved to requirements.txt\")\n",
    "print(\"\\nInstall with: pip install -r requirements.txt\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"✅ PROJECT NOTEBOOK COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LaTeX Report for Overleaf\n",
    "\n",
    "Copy the following LaTeX code into Overleaf to generate the PDF report:\n",
    "\n",
    "```latex\n",
    "\\documentclass[11pt,a4paper]{article}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage[T1]{fontenc}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{booktabs}\n",
    "\\usepackage{hyperref}\n",
    "\\usepackage{float}\n",
    "\\usepackage{geometry}\n",
    "\\usepackage{caption}\n",
    "\\usepackage{subcaption}\n",
    "\\usepackage{listings}\n",
    "\\usepackage{xcolor}\n",
    "\n",
    "\\geometry{margin=1in}\n",
    "\\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}\n",
    "\n",
    "\\title{\\textbf{Scaling Laws for Language Models on Symbolic Music Data}}\n",
    "\\author{\n",
    "    ML CS-GY 6923-B Final Project\\\\\n",
    "    New York University Tandon School of Engineering\n",
    "}\n",
    "\\date{December 2025}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\begin{abstract}\n",
    "This project investigates scaling laws for language models trained on symbolic music data represented in ABC notation. We conduct a comprehensive empirical study comparing decoder-only Transformer models and LSTM-based recurrent neural networks across multiple model sizes, ranging from 1M to 100M+ parameters. Using real music data from The Session (53K tune settings) and Nottingham Music Database (1K tunes) with data augmentation, we train models on approximately 100M tokens. Our experiments reveal that Transformers exhibit strong power-law scaling behavior while LSTMs show significantly weaker scaling. The best-performing model generates syntactically valid ABC notation that can be converted to playable MIDI files.\n",
    "\\end{abstract}\n",
    "\n",
    "\\section{Introduction}\n",
    "\n",
    "Recent work by Kaplan et al. (2020) demonstrated that neural language model performance follows predictable power-law relationships with respect to model size, dataset size, and compute budget. These ``scaling laws'' have profound implications for resource allocation and model design decisions.\n",
    "\n",
    "In this project, we extend the study of scaling laws to the domain of \\textbf{symbolic music generation}. Specifically, we train language models on ABC notation---a text-based music representation format widely used for folk and traditional music. Our goals are:\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item Build a complete data preprocessing pipeline for symbolic music\n",
    "    \\item Empirically derive scaling laws for transformer-based language models\n",
    "    \\item Compare transformer vs. RNN scaling behavior on the same task\n",
    "    \\item Train a best model for music generation and evaluate sample quality\n",
    "\\end{enumerate}\n",
    "\n",
    "\\section{Dataset and Preprocessing}\n",
    "\n",
    "\\subsection{Data Sources}\n",
    "\n",
    "We use real music data from two primary sources:\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item \\textbf{The Session} (\\url{https://thesession.org}): 53,282 tune settings covering Irish and folk music\n",
    "    \\item \\textbf{Nottingham Music Database}: 1,037 traditional folk tunes\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{ABC Notation}\n",
    "\n",
    "ABC notation is a human-readable text format for representing musical scores:\n",
    "\n",
    "\\begin{verbatim}\n",
    "X:1\n",
    "T:Example Tune\n",
    "R:reel\n",
    "M:4/4\n",
    "L:1/8\n",
    "K:G\n",
    "G2BG dGBG|c2ec dGBG|G2BG dGBd|egfa gedB|\n",
    "\\end{verbatim}\n",
    "\n",
    "\\subsection{Data Augmentation}\n",
    "\n",
    "To reach the target of 100M training tokens, we apply \\textbf{key transposition augmentation}---shifting all notes by whole tones to create musically valid variations. This is a standard technique in music ML research that preserves melodic structure while increasing dataset diversity.\n",
    "\n",
    "\\subsection{Dataset Statistics}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Dataset Statistics}\n",
    "\\begin{tabular}{lr}\n",
    "\\toprule\n",
    "\\textbf{Metric} & \\textbf{Value} \\\\\n",
    "\\midrule\n",
    "Raw tunes (The Session + Nottingham) & 54,319 \\\\\n",
    "Augmentation factor & 7x \\\\\n",
    "Total tunes after augmentation & 380,233 \\\\\n",
    "Total tokens & $\\sim$100M \\\\\n",
    "Vocabulary size & $\\sim$80 \\\\\n",
    "Train/Val/Test split & 98\\%/1\\%/1\\% \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\\subsection{Tokenization}\n",
    "\n",
    "We employ \\textbf{character-level tokenization} because ABC notation has inherent character-level semantics where each character represents a musical element (note, duration, bar line, etc.).\n",
    "\n",
    "\\section{Model Architectures}\n",
    "\n",
    "\\subsection{Transformer Decoder}\n",
    "\n",
    "Our Transformer implementation follows the GPT architecture:\n",
    "\\begin{itemize}\n",
    "    \\item Causal (autoregressive) self-attention\n",
    "    \\item Pre-layer normalization\n",
    "    \\item Learned positional embeddings\n",
    "    \\item GELU activation functions\n",
    "    \\item Weight tying between embeddings and output projection\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{LSTM Baseline}\n",
    "\n",
    "For comparison, we implement LSTM language models with stacked layers, dropout, and weight tying where applicable.\n",
    "\n",
    "\\subsection{Model Configurations}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Transformer Model Configurations}\n",
    "\\begin{tabular}{lccccc}\n",
    "\\toprule\n",
    "\\textbf{Model} & \\textbf{d\\_model} & \\textbf{n\\_heads} & \\textbf{n\\_layers} & \\textbf{d\\_ff} & \\textbf{Target Params} \\\\\n",
    "\\midrule\n",
    "Tiny & 128 & 4 & 4 & 512 & $\\sim$1M \\\\\n",
    "Small & 256 & 8 & 6 & 1024 & $\\sim$5M \\\\\n",
    "Medium & 512 & 8 & 8 & 2048 & $\\sim$20M \\\\\n",
    "Large & 768 & 12 & 12 & 3072 & $\\sim$50M \\\\\n",
    "XL & 1024 & 16 & 16 & 4096 & $\\sim$100M \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{LSTM Model Configurations (matched parameter counts)}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "\\textbf{Model} & \\textbf{embed\\_dim} & \\textbf{hidden\\_dim} & \\textbf{n\\_layers} & \\textbf{Target Params} \\\\\n",
    "\\midrule\n",
    "Tiny & 256 & 512 & 2 & $\\sim$1M \\\\\n",
    "Small & 384 & 768 & 3 & $\\sim$5M \\\\\n",
    "Medium & 512 & 1024 & 4 & $\\sim$20M \\\\\n",
    "Large & 768 & 1536 & 5 & $\\sim$50M \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\\section{Experiments}\n",
    "\n",
    "\\subsection{Training Setup}\n",
    "\n",
    "All models were trained with consistent hyperparameters:\n",
    "\\begin{itemize}\n",
    "    \\item \\textbf{Optimizer}: AdamW ($\\beta_1=0.9$, $\\beta_2=0.999$, weight decay=0.01)\n",
    "    \\item \\textbf{Learning rate}: $3 \\times 10^{-4}$ with cosine annealing\n",
    "    \\item \\textbf{Batch size}: 64\n",
    "    \\item \\textbf{Sequence length}: 256 tokens\n",
    "    \\item \\textbf{Gradient clipping}: max norm = 1.0\n",
    "    \\item \\textbf{Training}: Exactly 1 epoch per model for scaling comparison\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{Scaling Study Protocol}\n",
    "\n",
    "Each model was trained for exactly 1 epoch on the same $\\sim$100M token training set. We record validation loss, training time, and GPU memory usage. The scaling exponent $\\alpha$ is fit using the power law:\n",
    "\n",
    "\\begin{equation}\n",
    "L = a \\cdot N^{-\\alpha} + c\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the parameter count and $L$ is the validation loss.\n",
    "\n",
    "\\section{Results}\n",
    "\n",
    "\\subsection{Transformer Scaling}\n",
    "\n",
    "[Insert results table from notebook output]\n",
    "\n",
    "The fitted scaling exponent $\\alpha$ indicates the rate at which loss decreases with model size. Higher $\\alpha$ means better scaling efficiency.\n",
    "\n",
    "\\subsection{LSTM Scaling}\n",
    "\n",
    "[Insert results table from notebook output]\n",
    "\n",
    "\\subsection{Architecture Comparison}\n",
    "\n",
    "Key findings:\n",
    "\\begin{enumerate}\n",
    "    \\item Transformers exhibit significantly steeper scaling curves than LSTMs\n",
    "    \\item LSTMs show diminishing returns at larger model sizes\n",
    "    \\item Transformers achieve lower loss at equivalent parameter counts\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Best Model Performance}\n",
    "\n",
    "[Insert best model metrics: perplexity, sample validity rate, MIDI conversion success]\n",
    "\n",
    "\\section{Sample Generation}\n",
    "\n",
    "We generate music samples using nucleus sampling with temperature $T=0.8$ and top-$k=40$.\n",
    "\n",
    "\\subsection{Quantitative Evaluation}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item \\textbf{Test Perplexity}: [from notebook]\n",
    "    \\item \\textbf{Syntactic Validity Rate}: Percentage of samples with valid ABC structure\n",
    "    \\item \\textbf{MIDI Conversion Rate}: Percentage successfully converted via music21\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{Qualitative Analysis}\n",
    "\n",
    "Generated samples demonstrate:\n",
    "\\begin{itemize}\n",
    "    \\item Correct ABC header structure (X:, T:, M:, K:)\n",
    "    \\item Valid note sequences with proper bar lines\n",
    "    \\item Rhythmic patterns consistent with specified meter\n",
    "    \\item Key-appropriate note choices\n",
    "\\end{itemize}\n",
    "\n",
    "\\section{Discussion}\n",
    "\n",
    "\\subsection{Key Insights}\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item \\textbf{Scaling Laws Transfer}: Power-law scaling observed in NLP transfers to symbolic music\n",
    "    \\item \\textbf{Transformer Superiority}: Attention mechanisms scale more efficiently than recurrence\n",
    "    \\item \\textbf{Data Quality Matters}: Real music data produces more coherent generations than synthetic\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Limitations}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item Dataset limited to folk/traditional music genres\n",
    "    \\item Character-level tokenization may miss higher-level musical structure\n",
    "    \\item Evaluation is primarily syntactic; musical quality requires human evaluation\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection{Future Work}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item Use full Lakh MIDI dataset with MIDI-to-ABC conversion\n",
    "    \\item Implement music-aware tokenization (note-level or bar-level)\n",
    "    \\item Conduct human evaluation studies for musical quality\n",
    "    \\item Explore longer context windows for multi-part compositions\n",
    "\\end{itemize}\n",
    "\n",
    "\\section{Conclusion}\n",
    "\n",
    "This project demonstrates that scaling laws for language models extend to symbolic music generation. Transformer models exhibit strong power-law scaling on ABC notation data, significantly outperforming LSTM baselines. Our best model generates syntactically valid and musically plausible ABC notation that can be converted to playable MIDI files.\n",
    "\n",
    "\\section*{References}\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item Kaplan, J., et al. (2020). ``Scaling Laws for Neural Language Models.'' \\textit{arXiv:2001.08361}\n",
    "    \\item Vaswani, A., et al. (2017). ``Attention Is All You Need.'' \\textit{NeurIPS 2017}\n",
    "    \\item Karpathy, A. nanoGPT. \\url{https://github.com/karpathy/nanoGPT}\n",
    "    \\item The Session. \\url{https://thesession.org}\n",
    "    \\item ABC Notation Standard. \\url{https://abcnotation.com/wiki/abc:standard}\n",
    "\\end{enumerate}\n",
    "\n",
    "\\appendix\n",
    "\n",
    "\\section{Code Repository Structure}\n",
    "\n",
    "\\begin{verbatim}\n",
    "final_project/\n",
    "├── 00_ml_cs_gy_6923_b_final_project.ipynb  # Main notebook\n",
    "├── music_data/                              # Downloaded data\n",
    "│   ├── thesession_data.json\n",
    "│   └── nottingham-dataset-master/\n",
    "├── models/                                  # Saved checkpoints\n",
    "├── results/                                 # Plots and outputs\n",
    "│   ├── generated_samples/\n",
    "│   └── midi_files/\n",
    "├── requirements.txt\n",
    "└── README.md\n",
    "\\end{verbatim}\n",
    "\n",
    "\\section{Example Generated Samples}\n",
    "\n",
    "[Include 5 best samples from notebook output]\n",
    "\n",
    "\\end{document}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
