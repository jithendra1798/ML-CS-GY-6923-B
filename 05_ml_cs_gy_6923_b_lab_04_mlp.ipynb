{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jithendra1798/ML-CS-GY-6923-B/blob/main/05_ml_cs_gy_6923_b_lab_04_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 4: Fitting an MLP to regression data\n",
        "\n",
        "In this lab, you will implement and a train a simple neural network in PyTorch. Then, you will study how the function approximation capabilities of this model are affected by the model width and depth."
      ],
      "metadata": {
        "id": "-QJsP7161ZP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JTHGVMLEwYv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some regression data using a complicated function. There will be no test data, just the train set that we will try to fit."
      ],
      "metadata": {
        "id": "NZ3koAHC13g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 0.2 + 0.4 * x**2 + 0.3 * x * torch.sin(15 * x)+ 0.05 * torch.cos(50*x) - 0.5\n",
        "\n",
        "x = torch.linspace(0, 1, 1000)\n",
        "fs = f(x)\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "\n",
        "ax.plot(x, fs, \"-b\")\n",
        "ax.grid()\n",
        "ax.set_ylim([-0.5, 0.5])\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n"
      ],
      "metadata": {
        "id": "VotpjXm2FGPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the function below. See the docstring for detailed instructions. Make sure that the training works and you get a reasonable fit when you run the function with the provided parameters."
      ],
      "metadata": {
        "id": "vQd_wGz32Dd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    x: torch.Tensor,\n",
        "    y: torch.Tensor,\n",
        "    width: int,\n",
        "    num_hidden_layers: int,\n",
        "    epochs: int = 1000,\n",
        "    activation=nn.ReLU,\n",
        "    lr: float=0.01,\n",
        "    ):\n",
        "  \"\"\"\n",
        "    Train a fully connected neural network for regression tasks using PyTorch.\n",
        "\n",
        "    This function constructs a feedforward neural network with a specified architecture,\n",
        "    trains it using the Adam optimizer and mean squared error loss, and returns both\n",
        "    the trained model and its predictions on the input data.\n",
        "\n",
        "    Architecture:\n",
        "        - Input layer: Linear(1, width) → Activation\n",
        "        - Hidden layers: [Linear(width, width) → Activation] × num_hidden_layers\n",
        "        - Output layer: Linear(width, 1) (no activation)\n",
        "\n",
        "    The network expects 1-dimensional input features and produces scalar outputs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "        Input training data of shape (n_samples, 1) or (n_samples,).\n",
        "        Each sample should contain a single feature value.\n",
        "\n",
        "    y : torch.Tensor\n",
        "        Target training labels of shape (n_samples,).\n",
        "        Ground truth values for the regression task.\n",
        "\n",
        "    width : int\n",
        "        Number of neurons in each hidden layer.\n",
        "\n",
        "    num_hidden_layers : int\n",
        "        Number of hidden layers in the network (not counting input/output layers).\n",
        "\n",
        "    epochs : int, optional (default=1000)\n",
        "        Number of complete passes through the training dataset.\n",
        "\n",
        "    activation : torch.nn.Module class, optional (default=nn.ReLU)\n",
        "        Activation function class to use after each linear layer except the output.\n",
        "        Common choices: nn.ReLU, nn.Sigmoid, nn.Tanh, nn.LeakyReLU.\n",
        "        Should be passed as a class (not an instance), e.g., nn.ReLU, not nn.ReLU().\n",
        "\n",
        "    lr : float, optional (default=0.01)\n",
        "        Learning rate for the Adam optimizer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : torch.nn.Sequential\n",
        "        The trained neural network model in evaluation mode. Can be used for\n",
        "        inference on new data via model(new_x).\n",
        "\n",
        "    final_pred : torch.Tensor\n",
        "        Model predictions on the input data x, detached from the computation graph.\n",
        "        Shape: (n_samples,). Useful for immediate visualization or evaluation.\n",
        "\n",
        "    Training Details\n",
        "    ----------------\n",
        "    - Optimizer: Adam\n",
        "    - Loss function: Mean Squared Error (MSE)\n",
        "\n",
        "  \"\"\"\n",
        "  model = ...\n",
        "  print(model)\n",
        "\n",
        "  model.train()\n",
        "  optimizer = ...\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      ...\n",
        "\n",
        "  model.eval()\n",
        "  final_pred = ...\n",
        "  return model, final_pred\n",
        "\n",
        "\n",
        "_, y_pred = train_model(x[:, None], fs, width=256, num_hidden_layers=1)\n",
        "\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "\n",
        "ax.plot(x, fs, \"-b\")\n",
        "ax.plot(x, y_pred, \"-r\")\n",
        "ax.grid()\n",
        "ax.set_ylim([-0.5, 0.5])\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n"
      ],
      "metadata": {
        "id": "kRnkeDMvuWfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, try the following values of width and depth:\n",
        "- `num_hidden_layers in [1, 3]`\n",
        "- `width in [1, 16, 64]`\n",
        "\n",
        "Sweep both parameters simultaneously, so you have 6 settings in total. Present your results as a $2 \\times 3$ grid of plots.\n",
        "\n",
        "Discuss your results in a few sentences."
      ],
      "metadata": {
        "id": "_dhdhp2ey0D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = {}\n",
        "\n",
        "for num_hidden_layers in [1, 3]:\n",
        "  for width in [1, 16, 64]:\n",
        "    ..."
      ],
      "metadata": {
        "id": "JEdXMVN-JlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, arr = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "a-yYvit1z1UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TdYV0i1J0aAQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}